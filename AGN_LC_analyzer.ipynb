{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf65afb",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Times New Roman', cursive, sans-serif; font-weight: bold; color: brown;font-size: 34px;\">Importing Modules</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16940a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import integrate\n",
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy.optimize as spo\n",
    "import csv\n",
    "from optimparallel import minimize_parallel\n",
    "import time\n",
    "from scipy.interpolate import Rbf\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from scipy.optimize import minimize\n",
    "from scipy.integrate import trapz\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import norm\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.colors as mcolors\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb553349",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Times New Roman', cursive, sans-serif; font-weight: bold; color: red;font-size: 48px;\">Simulations</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd05af",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Verdana', cursive, sans-serif; font-weight: bold; color: green;font-size: 38px;\">For the Power law PSD Model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020209d",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Palatino', cursive, sans-serif; font-weight: bold; color: blue;font-size: 30px;\">Simple Case</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f962b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "n_realizations = 100 # number of realizations to simulate\n",
    "results_list = []\n",
    "\n",
    "for i in range(n_realizations):\n",
    "\n",
    "    \n",
    "    np.random.seed(i)\n",
    "    \n",
    "    gamma=2\n",
    "    alpha1=1.\n",
    "\n",
    "    N=600 # number of points in simulated light curve\n",
    "    step=1 # time sampling of simulated light curve\n",
    "\n",
    "    f_dc=0.1\n",
    "\n",
    "#noise level\n",
    "\n",
    "    noise_level=0.03\n",
    "\n",
    "    sigma_F=f_dc*noise_level\n",
    "\n",
    "#Fourier frequencies\n",
    "\n",
    "    w=2*(np.pi)*np.linspace(1,N,N)/N/step\n",
    "\n",
    "#Power at each frequency depending on power spectrum model. \n",
    "\n",
    "\n",
    "    def power (gamma):\n",
    "    \n",
    "        p= w**(-gamma)\n",
    "             \n",
    "        return p\n",
    "\n",
    "\n",
    "#Generate two sets of normally distributed random numbers \n",
    "    \n",
    "    aa = np.random.randn(len(w))\n",
    "    bb = np.random.randn(len(w))\n",
    "    \n",
    "# Light curve in frequency domain  = SQRT(p/2) * (a + b*i)\n",
    "\n",
    "    f_w = np.sqrt(0.5*power(gamma))*(aa + bb*1j)\n",
    "\n",
    "    f_w[0]=f_dc * len(w)**0.5\n",
    "\n",
    "#generating noise\n",
    "\n",
    "    n=np.ones(len(w))*(sigma_F**2)\n",
    "\n",
    "    n_w=np.sqrt(0.5*n)*(aa + bb*1.j)\n",
    "\n",
    "    n_w[0] = np.random.randn(1)*sigma_F\n",
    "\n",
    "# Generating the total flux (Signal + noise)\n",
    "\n",
    "    f1_w = alpha1 * f_w\n",
    "    phi_w = f1_w \n",
    "    F_w = phi_w+n_w\n",
    "\n",
    "    f_t = np.real(np.fft.ifft(f_w[:], norm='ortho'))\n",
    "\n",
    "    Ft = np.real(np.fft.ifft(F_w[:], norm='ortho'))\n",
    "\n",
    "\n",
    "    def Sigma_phi(alpha1, gamma):\n",
    "        Sigma_phi_ = (alpha1**2) * power(gamma)\n",
    "        return Sigma_phi_\n",
    "\n",
    "\n",
    "    \n",
    "    def cov_mat(alpha1, gamma):\n",
    "       dt = np.linspace(0, N, N)\n",
    "       cov = Sigma_phi(alpha1, gamma)\n",
    "    \n",
    "    # calculate sigma for all dt values\n",
    "       sigma = 2 * np.trapz(cov * np.cos(w * dt[:, np.newaxis]), w, axis=1)\n",
    "    \n",
    "    # add noise to the first element of sigma\n",
    "       sigma[0] += sigma_F**2\n",
    "    \n",
    "    # create the covariance matrix by subtracting the absolute difference between indices\n",
    "       ID = np.arange(sigma.size)\n",
    "       sigma = sigma[np.abs(ID - ID[:, None])]\n",
    "    \n",
    "       return sigma\n",
    "\n",
    "\n",
    "    \n",
    "    def log_like_F (alpha1,gamma):\n",
    "    \n",
    "        mu=np.mean(Ft)\n",
    "      \n",
    "        data=Ft-mu\n",
    "    \n",
    "        inv=np.linalg.inv(cov_mat(alpha1,gamma))\n",
    "    \n",
    "        sign,logdet=np.linalg.slogdet(2*np.pi*cov_mat( alpha1, gamma))\n",
    "    \n",
    "        lh=-0.5*( logdet + np.transpose(data) @ inv @ data )\n",
    "    \n",
    "        return lh\n",
    "\n",
    "\n",
    "\n",
    "    def mnz(args):\n",
    "        alpha1, gamma = args\n",
    "        return -log_like_F(alpha1, gamma) + log_like_F(1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    g1 = np.arange(1.5, 2.5, 0.025)\n",
    "\n",
    "    diffe = np.zeros(len(g1))\n",
    "    amp = np.zeros(len(g1))\n",
    "\n",
    "    results = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.2, g1_), bounds=((0.1, 0.4), (g1_, g1_)))\n",
    "        for g1_ in g1\n",
    "    )\n",
    "\n",
    "    for j, res1 in enumerate(results):\n",
    "        amp[j] = res1.x[0]\n",
    "        diffe[j] = -log_like_F(amp[j], g1[j]) + log_like_F(1, 1)\n",
    "\n",
    "    results_list.append({'amp': amp, 'diffe': diffe})\n",
    "\n",
    "    rows=zip(g1,amp,diffe)\n",
    "\n",
    "    with open ('pl_loop.dat', 'a') as f:\n",
    "        writer=csv.writer(f, delimiter=' ')\n",
    "        for i in rows:\n",
    "            writer.writerow(i)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526d49a",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Palatino', cursive, sans-serif; font-weight: bold; color: blue;font-size: 30px;\">LC analysis in Segments</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0147c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "n_realizations = 100\n",
    "results_list = []\n",
    "\n",
    "for i in range(n_realizations):\n",
    "\n",
    "    \n",
    "    np.random.seed(i)\n",
    "    \n",
    "    gamma=2\n",
    "    alpha1=1.\n",
    "\n",
    "    N=600 # number of points in simulated light curve\n",
    "    step=1 # time sampling of simulated light curve\n",
    "\n",
    "    f_dc=0.1\n",
    "\n",
    "#noise term\n",
    "\n",
    "    noise_level=0.03\n",
    "\n",
    "    sigma_F=f_dc*noise_level\n",
    "\n",
    "#Fourier frequencies\n",
    "\n",
    "    w=2*(np.pi)*np.linspace(1,N,N)/N/step\n",
    "\n",
    "#Power at each frequency depending on power spectrum model. \n",
    "\n",
    "\n",
    "    def power (gamma):\n",
    "    \n",
    "        p= w**(-gamma)\n",
    "             \n",
    "        return p\n",
    "\n",
    "\n",
    "#Generate two sets of normally distributed random numbers \n",
    "    \n",
    "    aa = np.random.randn(len(w))\n",
    "    bb = np.random.randn(len(w))\n",
    "    \n",
    "# Light curve in frequency domain  = SQRT(p/2) * (a + b*i)\n",
    "\n",
    "    f_w = np.sqrt(0.5*power(gamma))*(aa + bb*1j)\n",
    "\n",
    "    f_w[0]=f_dc * len(w)**0.5\n",
    "\n",
    "#generating noise\n",
    "\n",
    "    n=np.ones(len(w))*(sigma_F**2)\n",
    "\n",
    "    n_w=np.sqrt(0.5*n)*(aa + bb*1.j)\n",
    "\n",
    "    n_w[0] = np.random.randn(1)*sigma_F\n",
    "\n",
    "# Generating the total flux (Signal + noise)\n",
    "\n",
    "    f1_w = alpha1 * f_w\n",
    "    phi_w = f1_w \n",
    "    F_w = phi_w+n_w\n",
    "\n",
    "    f_t = np.real(np.fft.ifft(f_w[:], norm='ortho'))\n",
    "\n",
    "    F_t = np.real(np.fft.ifft(F_w[:], norm='ortho'))\n",
    "\n",
    "    lim=200\n",
    "    low=0\n",
    "    high=1\n",
    "\n",
    "    Ft=F_t[low*lim:high*lim]\n",
    "\n",
    "\n",
    "    def Sigma_phi(alpha1, gamma):\n",
    "        Sigma_phi_ = (alpha1**2) * power(gamma)\n",
    "        return Sigma_phi_\n",
    "  \n",
    "    \n",
    "    def cov_mat(alpha1, gamma):\n",
    "       \n",
    "        dt=np.linspace(0,lim,lim)\n",
    "        \n",
    "        cov=Sigma_phi(alpha1, gamma)\n",
    "    \n",
    "    # calculate sigma for all dt values\n",
    "        sigma = 2 * np.trapz(cov * np.cos(w * dt[:, np.newaxis]), w, axis=1)\n",
    "    \n",
    "    # add noise to the first element of sigma\n",
    "        sigma[0] += sigma_F**2\n",
    "    \n",
    "    # create the covariance matrix by subtracting the absolute difference between indices\n",
    "        ID = np.arange(sigma.size)\n",
    "        sigma = sigma[np.abs(ID - ID[:, None])]\n",
    "    \n",
    "        return sigma\n",
    "\n",
    "    \n",
    "    def log_like_F (alpha1,gamma):\n",
    "    \n",
    "        mu=np.mean(Ft)\n",
    "      \n",
    "        data=Ft-mu\n",
    "    \n",
    "        inv=np.linalg.inv(cov_mat(alpha1,gamma))\n",
    "    \n",
    "        sign,logdet=np.linalg.slogdet(2*np.pi*cov_mat( alpha1, gamma))\n",
    "    \n",
    "        lh=-0.5*( logdet + np.transpose(data) @ inv @ data )\n",
    "    \n",
    "        return lh\n",
    "\n",
    "\n",
    "    def mnz(args):\n",
    "        alpha1, gamma = args\n",
    "        return -log_like_F(alpha1, gamma) + log_like_F(1, 1)\n",
    "\n",
    "\n",
    "    g1 = np.arange(1.5, 2.5, 0.025)\n",
    "\n",
    "    diffe = np.zeros(len(g1))\n",
    "    amp = np.zeros(len(g1))\n",
    "\n",
    "    results = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.2, g1_), bounds=((0.1, 0.4), (g1_, g1_)))\n",
    "        for g1_ in g1\n",
    "    )\n",
    "\n",
    "    for j, res1 in enumerate(results):\n",
    "        amp[j] = res1.x[0]\n",
    "        diffe[j] = -log_like_F(amp[j], g1[j]) + log_like_F(1, 1)\n",
    "\n",
    "    results_list.append({'amp': amp, 'diffe': diffe})\n",
    "\n",
    "    rows=zip(g1,diffe)\n",
    "\n",
    "    with open ('1.dat', 'a') as f:\n",
    "        writer=csv.writer(f, delimiter=' ')\n",
    "        for i in rows:\n",
    "            writer.writerow(i)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac41c088",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Palatino', cursive, sans-serif; font-weight: bold; color: blue;font-size: 30px;\">Effect of Gaps and Interpolation Schemes</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55338cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "n_realizations = 100\n",
    "results_list = []\n",
    "\n",
    "for i in range(n_realizations):\n",
    "\n",
    "    \n",
    "    np.random.seed(i)\n",
    "    \n",
    "    gamma=2\n",
    "    alpha1=1.\n",
    "\n",
    "    N=600 # number of points in simulated light curve\n",
    "    step=1 # time sampling of simulated light curve\n",
    "\n",
    "    f_dc=0.1\n",
    "\n",
    "#noise term\n",
    "\n",
    "    noise_level=0.03\n",
    "\n",
    "    sigma_F=f_dc*noise_level\n",
    "\n",
    "#Fourier frequencies\n",
    "\n",
    "    w=2*(np.pi)*np.linspace(1,N,N)/N/step\n",
    "\n",
    "#Power at each frequency depending on power spectrum model. \n",
    "\n",
    "\n",
    "    def power (gamma):\n",
    "    \n",
    "        p= w**(-gamma)\n",
    "             \n",
    "        return p\n",
    "\n",
    "\n",
    "#Generate two sets of normally distributed random numbers \n",
    "    \n",
    "    aa = np.random.randn(len(w))\n",
    "    bb = np.random.randn(len(w))\n",
    "    \n",
    "# Light curve in frequency domain  = SQRT(p/2) * (a + b*i)\n",
    "\n",
    "    f_w = np.sqrt(0.5*power(gamma))*(aa + bb*1j)\n",
    "\n",
    "    f_w[0]=f_dc * len(w)**0.5\n",
    "\n",
    "#generating noise\n",
    "\n",
    "    n=np.ones(len(w))*(sigma_F**2)\n",
    "\n",
    "    n_w=np.sqrt(0.5*n)*(aa + bb*1.j)\n",
    "\n",
    "    n_w[0] = np.random.randn(1)*sigma_F\n",
    "\n",
    "# Generating the total flux (Signal + noise)\n",
    "\n",
    "    f1_w = alpha1 * f_w\n",
    "    phi_w = f1_w \n",
    "    F_w = phi_w+n_w\n",
    "\n",
    "    f_t = np.real(np.fft.ifft(f_w[:], norm='ortho'))\n",
    "\n",
    "    F_t = np.real(np.fft.ifft(F_w[:], norm='ortho'))\n",
    "\n",
    "# section to remove points and interpolation\n",
    "\n",
    "    T=np.linspace(0,N,N)\n",
    "\n",
    "    # Calculate the number of points to remove\n",
    "   \n",
    "    num_points = int(len(T) * 0.1)\n",
    "\n",
    "    # Randomly select indices to remove with gaps ranging from 1 to 10 points\n",
    "    remove_indices = []\n",
    "    num_removed = 0\n",
    "    while num_removed < num_points:\n",
    "        index = np.random.randint(0, len(T) - 10)\n",
    "        gap = np.random.randint(1, 11)\n",
    "        remove_indices.extend(range(index, min(index + gap, len(T))))\n",
    "        num_removed += gap\n",
    "\n",
    "    # Remove the selected data points from the time series and the flux array\n",
    "    t4 = np.delete(T, remove_indices)\n",
    "    Ft4 = np.delete(F_t, remove_indices)\n",
    "    \n",
    "    \n",
    "\n",
    "#    interpolation schemes\n",
    "\n",
    " #   f = interpolate.interp1d(t4,Ft4,kind='linear',fill_value='extrapolate')\n",
    " #    f = Rbf(t4,Ft4,function='cubic',fill_value=1e+10)\n",
    "    # Ft=f(T)\n",
    " \n",
    "#  This section is for the Gaussian Process Interpolation \n",
    " \n",
    "# Define the Kriging model with Matern kernel\n",
    "   \n",
    "    kernel = Matern(length_scale=0.2, nu=2.5)\n",
    "    model = GaussianProcessRegressor(kernel=kernel)\n",
    "    \n",
    "    # Train the model\n",
    "    \n",
    "    model.fit(t4.reshape(-1, 1), Ft4)\n",
    "    Ft=model.predict(T.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    def Sigma_phi(alpha1, gamma):\n",
    "        Sigma_phi_ = (alpha1**2) * power(gamma)\n",
    "        return Sigma_phi_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def cov_mat(alpha1, gamma):\n",
    "        dt = np.linspace(0, N, N)\n",
    "        cov = Sigma_phi(alpha1, gamma)\n",
    "\n",
    "# calculate sigma for all dt values\n",
    "        sigma = 2 * np.trapz(cov * np.cos(w * dt[:, np.newaxis]), w, axis=1)\n",
    "\n",
    "# add noise to the first element of sigma\n",
    "        sigma[0] += sigma_F**2\n",
    "\n",
    "# create the covariance matrix by subtracting the absolute difference between indices\n",
    "        ID = np.arange(sigma.size)\n",
    "        sigma = sigma[np.abs(ID - ID[:, None])]\n",
    "\n",
    "        return sigma\n",
    "\n",
    "    \n",
    "    def log_like_F (alpha1,gamma):\n",
    "    \n",
    "        mu=np.mean(Ft)\n",
    "      \n",
    "        data=Ft-mu\n",
    "    \n",
    "        inv=np.linalg.inv(cov_mat(alpha1,gamma))\n",
    "    \n",
    "        sign,logdet=np.linalg.slogdet(2*np.pi*cov_mat( alpha1, gamma))\n",
    "    \n",
    "        lh=-0.5*( logdet + np.transpose(data) @ inv @ data )\n",
    "    \n",
    "        return lh\n",
    "\n",
    "\n",
    "    def mnz(args):\n",
    "        alpha1, gamma = args\n",
    "        return -log_like_F(alpha1, gamma) + log_like_F(1, 1)\n",
    "\n",
    "\n",
    "    g1 = np.arange(1.5, 2.5, 0.025)\n",
    "\n",
    "    diffe = np.zeros(len(g1))\n",
    "    amp = np.zeros(len(g1))\n",
    "\n",
    "    results = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.2, g1_), bounds=((0.1, 0.4), (g1_, g1_)))\n",
    "        for g1_ in g1\n",
    "    )\n",
    "\n",
    "    for j, res1 in enumerate(results):\n",
    "        amp[j] = res1.x[0]\n",
    "        diffe[j] = -log_like_F(amp[j], g1[j]) + log_like_F(1, 1)\n",
    "\n",
    "    results_list.append({'amp': amp, 'diffe': diffe})\n",
    "\n",
    "    rows=zip(g1,diffe)\n",
    "\n",
    " #   with open ('pl_lin.dat', 'a') as f:\n",
    " #   with open ('pl_cub.dat', 'a') as f:\n",
    "    with open ('pl_gas.dat', 'a') as f:\n",
    "        writer=csv.writer(f, delimiter=' ')\n",
    "        for i in rows:\n",
    "            writer.writerow(i)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d37f4",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Palatino', cursive, sans-serif; font-weight: bold; color: blue;font-size: 30px;\">Effect of Incorrect Noise Modeling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a928a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "n_realizations = 100\n",
    "results_list = []\n",
    "\n",
    "for i in range(n_realizations):\n",
    "\n",
    "    \n",
    "    np.random.seed(i)\n",
    "    \n",
    "    gamma=2\n",
    "    alpha1=1.\n",
    "\n",
    "    N=600 # number of points in simulated light curve\n",
    "    step=1 # time sampling of simulated light curve\n",
    "\n",
    "    f_dc=0.1\n",
    "\n",
    "#noise term\n",
    "\n",
    "    noise_level=0.03\n",
    "\n",
    "    sigma_F=f_dc*noise_level\n",
    "\n",
    "#Fourier frequencies\n",
    "\n",
    "    w=2*(np.pi)*np.linspace(1,N,N)/N/step\n",
    "\n",
    "#Power at each frequency depending on power spectrum model. \n",
    "\n",
    "\n",
    "    def power (gamma):\n",
    "    \n",
    "        p= w**(-gamma)\n",
    "             \n",
    "        return p\n",
    "\n",
    "\n",
    "#Generate two sets of normally distributed random numbers \n",
    "    \n",
    "    aa = np.random.randn(len(w))\n",
    "    bb = np.random.randn(len(w))\n",
    "    \n",
    "# Light curve in frequency domain  = SQRT(p/2) * (a + b*i)\n",
    "\n",
    "    f_w = np.sqrt(0.5*power(gamma))*(aa + bb*1j)\n",
    "\n",
    "    f_w[0]=f_dc * len(w)**0.5\n",
    "\n",
    "#generating noise\n",
    "\n",
    "    n=np.ones(len(w))*(sigma_F**2)\n",
    "\n",
    "    n_w=np.sqrt(0.5*n)*(aa + bb*1.j)\n",
    "\n",
    "    n_w[0] = np.random.randn(1)*sigma_F\n",
    "\n",
    "# Generating the total flux (Signal + noise)\n",
    "\n",
    "    f1_w = alpha1 * f_w\n",
    "    phi_w = f1_w \n",
    "    F_w = phi_w+n_w\n",
    "\n",
    "    f_t = np.real(np.fft.ifft(f_w[:], norm='ortho'))\n",
    "\n",
    "    Ft = np.real(np.fft.ifft(F_w[:], norm='ortho'))\n",
    "\n",
    "\n",
    "    def Sigma_phi(alpha1, gamma):\n",
    "        Sigma_phi_ = (alpha1**2) * power(gamma)\n",
    "        return Sigma_phi_\n",
    "\n",
    "\n",
    "    \n",
    "    def cov_mat(alpha1, gamma,sigma_F):\n",
    "       dt = np.linspace(0, N, N)\n",
    "       cov = Sigma_phi(alpha1, gamma)\n",
    "    \n",
    "    # calculate sigma for all dt values\n",
    "       sigma = 2 * np.trapz(cov * np.cos(w * dt[:, np.newaxis]), w, axis=1)\n",
    "    \n",
    "    # add noise to the first element of sigma\n",
    "       sigma[0] += sigma_F**2\n",
    "    \n",
    "    # create the covariance matrix by subtracting the absolute difference between indices\n",
    "       ID = np.arange(sigma.size)\n",
    "       sigma = sigma[np.abs(ID - ID[:, None])]\n",
    "    \n",
    "       return sigma\n",
    "\n",
    "\n",
    "    \n",
    "    def log_like_F (alpha1,gamma,sigma_F):\n",
    "    \n",
    "        mu=np.mean(Ft)\n",
    "      \n",
    "        data=Ft-mu\n",
    "    \n",
    "        inv=np.linalg.inv(cov_mat(alpha1,gamma,sigma_F))\n",
    "    \n",
    "        sign,logdet=np.linalg.slogdet(2*np.pi*cov_mat( alpha1, gamma, sigma_F))\n",
    "    \n",
    "        lh=-0.5*( logdet + np.transpose(data) @ inv @ data )\n",
    "    \n",
    "        return lh\n",
    "\n",
    "    def mnz(args):\n",
    "        alpha1, gamma, sigma_F= args\n",
    "        return -log_like_F(alpha1, gamma, sigma_F) + log_like_F(1, 1, sigma_F)\n",
    "\n",
    "\n",
    "    g1 = np.arange(1.5, 2.5, 0.025)\n",
    "\n",
    "    # Noise level from 0.5 to 2.0 in 10 log steps\n",
    "    \n",
    "    ss1, ss2, ss3, ss4, ss5, ss6, ss7, ss8, ss9, ss10 = np.logspace(0.5,2.0,num=10) * sigma_F\n",
    "    diffe1, diffe2, diffe3, diffe4, diffe5, diffe6, \n",
    "    diffe7, diffe8, diffe9, diffe10  = [np.zeros(len(g1)) for i in range(10)]\n",
    "    amp1, amp2, amp3, amp4, amp5, amp6, amp7, amp8, amp9, amp10 = [np.zeros(len(g1)) for i in range(10)]\n",
    "  \n",
    "    \n",
    "  \n",
    "    results1 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.2, g1_,ss1), bounds=((0.1, 10), (g1_, g1_),(ss1,ss1)))\n",
    "        for g1_ in g1\n",
    "    )\n",
    "\n",
    "    for j, res1 in enumerate(results1):\n",
    "        amp1[j] = res1.x[0]\n",
    "        diffe1[j] = -log_like_F(amp1[j], g1[j],ss1) + log_like_F(1, 1,ss1)\n",
    "        \n",
    "        \n",
    "    results2 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.2, g1_,ss2), bounds=((0.01, 10), (g1_, g1_),(ss2,ss2)))\n",
    "        for g1_ in g1\n",
    "    )\n",
    "\n",
    "    for j, res2 in enumerate(results2):\n",
    "        amp2[j] = res2.x[0]\n",
    "        diffe2[j] = -log_like_F(amp2[j], g1[j],ss2) + log_like_F(1, 1,ss2)\n",
    "        \n",
    "  \n",
    "    results3 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.2, g1_,ss3), bounds=((0.01, 10), (g1_, g1_),(ss3,ss3)))\n",
    "        for g1_ in g1\n",
    "    )\n",
    "\n",
    "    for j, res3 in enumerate(results3):\n",
    "        amp3[j] = res3.x[0]\n",
    "        diffe3[j] = -log_like_F(amp3[j], g1[j],ss3) + log_like_F(1, 1,ss3)\n",
    "        \n",
    "        \n",
    "    results4 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.2, g1_,ss4), bounds=((0.01, 10), (g1_, g1_),(ss4,ss4)))\n",
    "        for g1_ in g1\n",
    "    )\n",
    "\n",
    "    for j, res4 in enumerate(results4):\n",
    "        amp4[j] = res4.x[0]\n",
    "        diffe4[j] = -log_like_F(amp4[j], g1[j],ss4) + log_like_F(1, 1,ss4)\n",
    "        \n",
    "        \n",
    "    results5 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.2, g1_,ss5), bounds=((0.01, 10), (g1_, g1_),(ss5,ss5)))\n",
    "        for g1_ in g1\n",
    "    )\n",
    "\n",
    "    for j, res5 in enumerate(results5):\n",
    "        amp5[j] = res5.x[0]\n",
    "        diffe5[j] = -log_like_F(amp5[j], g1[j],ss5) + log_like_F(1, 1,ss5)\n",
    "        \n",
    "        \n",
    "    results6 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.2, g1_,ss6), bounds=((0.01, 10), (g1_, g1_),(ss6,ss6)))\n",
    "        for g1_ in g1\n",
    "    )\n",
    "\n",
    "    for j, res6 in enumerate(results6):\n",
    "        amp6[j] = res6.x[0]\n",
    "        diffe6[j] = -log_like_F(amp6[j], g1[j],ss6) + log_like_F(1, 1,ss6)\n",
    "        \n",
    "  \n",
    "    results7 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.2, g1_,ss7), bounds=((0.01, 10), (g1_, g1_),(ss7,ss7)))\n",
    "        for g1_ in g1\n",
    "    )\n",
    "\n",
    "    for j, res7 in enumerate(results7):\n",
    "        amp7[j] = res7.x[0]\n",
    "        diffe7[j] = -log_like_F(amp7[j], g1[j],ss7) + log_like_F(1, 1,ss7)\n",
    "        \n",
    "        \n",
    "    results8 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.2, g1_,ss8), bounds=((0.01, 10), (g1_, g1_),(ss8,ss8)))\n",
    "        for g1_ in g1\n",
    "    )\n",
    "\n",
    "    for j, res8 in enumerate(results8):\n",
    "        amp8[j] = res8.x[0]\n",
    "        diffe8[j] = -log_like_F(amp8[j], g1[j],ss8) + log_like_F(1, 1,ss8)\n",
    "        \n",
    "        \n",
    "    results9 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.2, g1_,ss8), bounds=((0.01, 10), (g1_, g1_),(ss9,ss9)))\n",
    "        for g1_ in g1\n",
    "    )\n",
    "\n",
    "    for j, res9 in enumerate(results9):\n",
    "        amp9[j] = res9.x[0]\n",
    "        diffe9[j] = -log_like_F(amp9[j], g1[j],ss9) + log_like_F(1, 1,ss9)\n",
    "        \n",
    "        \n",
    "    results10 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.2, g1_,ss10), bounds=((0.01, 10), (g1_, g1_),(ss10,ss10)))\n",
    "        for g1_ in g1\n",
    "    )\n",
    "\n",
    "    for j, res10 in enumerate(results10):\n",
    "        amp10[j] = res10.x[0]\n",
    "        diffe10[j] = -log_like_F(amp10[j], g1[j],ss10) + log_like_F(1, 1,ss10)\n",
    "        \n",
    "        \n",
    "        \n",
    "    rows=zip(np.round(g1,2),np.round(diffe1,2),np.round(diffe2,2),np.round(diffe3,2),np.round(diffe4,2),\n",
    "             np.round(diffe5,2),np.round(diffe6,2),np.round(diffe7,2),np.round(diffe8,2),\n",
    "             np.round(diffe9,2),np.round(diffe10,2))\n",
    "\n",
    "\n",
    "    with open ('pl_log_noise.dat', 'a') as f:\n",
    "        writer=csv.writer(f, delimiter=' ')\n",
    "        for i in rows:\n",
    "            writer.writerow(i)\n",
    "\n",
    "end = time.time()\n",
    "print((end - start)/3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2b18d",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Century Gothic', cursive, sans-serif; font-weight: bold; color: purple;font-size: 30px;\">Template for Plots</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32db463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#___________ SECTIONS __________________________\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=25) \n",
    "matplotlib.rc('ytick', labelsize=25) \n",
    "plt.rc('font', size=25)\n",
    "\n",
    "df = 2\n",
    "s = -0.5 * chi2.ppf(0.68, df)\n",
    "s2 = -0.5 * chi2.ppf(0.95, df)\n",
    "s3 = -0.5 * chi2.ppf(0.997, df)\n",
    "\n",
    "\n",
    "slope=np.loadtxt('1.dat')[:,0]\n",
    "l1=np.loadtxt('1.dat')[:,1]\n",
    "l2=np.loadtxt('2.dat')[:,1]\n",
    "l3=np.loadtxt('3.dat')[:,1]\n",
    "\n",
    "\n",
    "\n",
    "slp = np.arange(1.5, 2.5, 0.025)\n",
    "\n",
    "lf1 = np.zeros(len(slp))\n",
    "lf2 = np.zeros(len(slp))\n",
    "lf3 = np.zeros(len(slp))\n",
    "\n",
    "\n",
    "for i, sl_ in enumerate(slp):\n",
    "    lf1[i] = np.mean(l1[slope == sl_])\n",
    "    lf2[i] = np.mean(l2[slope == sl_])\n",
    "    lf3[i] = np.mean(l3[slope == sl_])\n",
    "\n",
    "\n",
    "\n",
    "sig1=lf1[lf1==np.min(lf1)]-s\n",
    "sig2=lf1[lf1==np.min(lf1)]-s2\n",
    "sig3=lf1[lf1==np.min(lf1)]-s3\n",
    "\n",
    "sig4=lf2[lf2==np.min(lf2)]-s\n",
    "sig5=lf2[lf2==np.min(lf2)]-s2\n",
    "sig6=lf2[lf2==np.min(lf2)]-s3\n",
    "\n",
    "sig7=lf3[lf3==np.min(lf3)]-s\n",
    "sig8=lf3[lf3==np.min(lf3)]-s2\n",
    "sig9=lf3[lf3==np.min(lf3)]-s3\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(25,20))\n",
    "plt.subplots_adjust(wspace = 0.5,hspace=0.5)\n",
    "axs[0, 0].plot(slp,lf1, linewidth=4,color='b',label=\"Mean of 100 simulations\")\n",
    "axs[0, 0].plot(slope[35*len(slp):36*len(slp)],l1[35*len(slp):36*len(slp)], linewidth=4,color='r',label=\"Single simulation\")\n",
    "axs[0, 0].set_title(' 0 to 200 days ')\n",
    "axs[0, 0].set_ylim([np.min(lf1)-1,np.min(lf1)+10])\n",
    "axs[0, 0].axhline(y=sig1,linewidth=4,color='black',linestyle='dashed')\n",
    "axs[0, 0].axhline(y=sig2,linewidth=4,color='black',linestyle='dashed')\n",
    "axs[0, 0].axhline(y=sig3,linewidth=4,color='black',linestyle='dashed')\n",
    "\n",
    "axs[0, 1].plot(slp,lf2, linewidth=4,color='b')\n",
    "axs[0, 1].plot(slope[35*len(slp):36*len(slp)],l1[35*len(slp):36*len(slp)], linewidth=4,color='r')\n",
    "axs[0, 1].set_title(' 200 to 400 days ')\n",
    "axs[0, 1].set_ylim([np.min(lf2)-1,np.min(lf2)+10])\n",
    "axs[0, 1].axhline(y=sig4,linewidth=4,color='black',linestyle='dashed')\n",
    "axs[0, 1].axhline(y=sig5,linewidth=4,color='black',linestyle='dashed')\n",
    "axs[0, 1].axhline(y=sig6,linewidth=4,color='black',linestyle='dashed')\n",
    "\n",
    "axs[1, 0].plot(slp,lf3, linewidth=4,color='b')\n",
    "axs[1, 0].plot(slope[35*len(slp):36*len(slp)],l3[35*len(slp):36*len(slp)], linewidth=4,color='r')\n",
    "axs[1, 0].set_title('400 to 600 days ')\n",
    "axs[1, 0].set_ylim([np.min(lf3)-1,np.min(lf3)+10])\n",
    "axs[1, 0].axhline(y=sig7,linewidth=4,color='black',linestyle='dashed')\n",
    "axs[1, 0].axhline(y=sig8,linewidth=4,color='black',linestyle='dashed')\n",
    "axs[1, 0].axhline(y=sig9,linewidth=4,color='black',linestyle='dashed')\n",
    "\n",
    "axs[1][0].set_position([0.35,0.1,0.32,0.28])\n",
    "axs[1, 1].set_visible(False)\n",
    "for ax in axs.flat:\n",
    "    ax.set_ylabel('$- \\ln \\mathcal{L}(F)$')\n",
    "    ax.set_xlabel(r'$\\gamma$')\n",
    "    ax.set_xlim(np.min(slp),np.max(slp))\n",
    "\n",
    "# Add legend outside the plot\n",
    "handles, labels = axs[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='center left', bbox_to_anchor=(0.5, 0.48))\n",
    "\n",
    "fig.savefig(\"sim_cut.pdf\")\n",
    "\n",
    "\n",
    "#__________    DIFFERENT INTERPOLATION    __________________________________\n",
    "\n",
    "\n",
    "slope=np.loadtxt('pl_cub.dat')[:,0]\n",
    "ll=np.loadtxt('pl_lin.dat')[:,1]\n",
    "lc=np.loadtxt('pl_cub.dat')[:,1]\n",
    "lg=np.loadtxt('pl_gas.dat')[:,1]\n",
    "lf=np.loadtxt('pl_full_matrix.dat')[:,2]\n",
    "\n",
    "\n",
    "slp=np.arange(1.5, 2.5, 0.025)\n",
    "\n",
    "lf_lin = np.zeros(len(slp))\n",
    "lf_cub = np.zeros(len(slp))\n",
    "lf_gas = np.zeros(len(slp))\n",
    "lf_full = np.zeros(len(slp))\n",
    "\n",
    "\n",
    "for i, sl_ in enumerate(slp):\n",
    "    \n",
    "    lf_lin[i] = np.mean(ll[slope == sl_])\n",
    "    lf_cub[i] = np.mean(lc[slope == sl_])\n",
    "    lf_gas[i] = np.mean(lg[slope == sl_])\n",
    "    lf_full[i] = np.mean(lf[slope == sl_])\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20), nrows=2, ncols=1, gridspec_kw={'height_ratios': [4, 2.9]})\n",
    "ax[0].plot(slp, lf_lin, linewidth=4,color='orange', label='Linear')\n",
    "ax[0].plot(slp, lf_cub, linewidth=4,color='b', label='Cubic')\n",
    "ax[0].plot(slp, lf_gas, linewidth=4,color='r', label='Gaussian')\n",
    "ax[0].plot(slp, lf_full, linewidth=4,color='g', label='Full Matrix')\n",
    "ax[0].set_ylabel('$ - \\ln \\mathcal{L}(F)$', fontsize=30.0)\n",
    "ax[0].legend(loc='upper center', fontsize=25)\n",
    "#ax[0].legend(loc='best', fontsize=25)\n",
    "ax[0].set_xlim(np.min(slp),np.max(slp))\n",
    "ax[0].grid(True)\n",
    "\n",
    "\n",
    "gam_lin=np.zeros(len(slope))\n",
    "gam_cub=np.zeros(len(slope))\n",
    "gam_gas=np.zeros(len(slope))\n",
    "gam_full=np.zeros(len(slope))\n",
    "\n",
    "length=len(slp)\n",
    "\n",
    "\n",
    "for i in range (0,len(slope),length):\n",
    "    gam_lin[i]=slope[i:i+length][ll[i:i+length]==np.min(ll[i:i+length])]\n",
    "    gam_cub[i]=slope[i:i+length][lc[i:i+length]==np.min(lc[i:i+length])]\n",
    "    gam_gas[i]=slope[i:i+length][lg[i:i+length]==np.min(lg[i:i+length])]\n",
    "    gam_full[i]=slope[i:i+length][lf[i:i+length]==np.min(lf[i:i+length])]\n",
    "\n",
    "    \n",
    "gam_lin=gam_lin[gam_lin!=0]\n",
    "gam_cub=gam_cub[gam_cub!=0]\n",
    "gam_gas=gam_gas[gam_gas!=0]\n",
    "gam_full=gam_full[gam_full!=0]\n",
    "\n",
    "\n",
    "\n",
    "bins = slp\n",
    "ax[1].hist(gam_lin, bins, color='orange', histtype='step', lw=6,label='Linear')\n",
    "ax[1].hist(gam_cub, bins, color='b', histtype='step', lw=6,label='Cubic')\n",
    "ax[1].hist(gam_gas, bins, color='r', histtype='step', lw=6,label='Gaussian')\n",
    "ax[1].hist(gam_full, bins, color='g', histtype='step', lw=6,label='Full Matrix')\n",
    "ax[1].set_xlabel(r'$\\gamma$', fontsize=30.0)\n",
    "ax[1].set_ylabel('Counts', fontsize=35)\n",
    "ax[1].legend(loc='best', fontsize=25)\n",
    "ax[1].set_xlim(np.min(slp),np.max(slp))\n",
    "fig.tight_layout()\n",
    "plt.savefig('pl_int.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# For the table: mean best fit gamma with 1 \\sigma std. dev\n",
    "\n",
    "print(round(np.mean(gam_lin),2),round(np.std(gam_lin),2))\n",
    "print(round(np.mean(gam_cub),2),round(np.std(gam_cub),2))\n",
    "print(round(np.mean(gam_gas),2),round(np.std(gam_gas),2))\n",
    "print(round(np.mean(gam_full),2),round(np.std(gam_full),2))\n",
    "\n",
    "\n",
    "#______________   POWER LAW   _______________________\n",
    "\n",
    "\n",
    "df = 2\n",
    "s = -0.5 * chi2.ppf(0.68, df)\n",
    "s2 = -0.5 * chi2.ppf(0.95, df)\n",
    "s3 = -0.5 * chi2.ppf(0.997, df)\n",
    "\n",
    "slope = np.loadtxt('pl_loop.dat')[:,0]\n",
    "l = np.loadtxt('pl_loop.dat')[:,2]\n",
    "\n",
    "slp = np.arange(1.5, 2.5, 0.025)\n",
    "\n",
    "lf = np.zeros(len(slp))\n",
    "ls = np.zeros(len(slp))\n",
    "\n",
    "for i, sl_ in enumerate(slp):\n",
    "    lf[i] = np.mean(l[slope == sl_])\n",
    "    ls[i] = np.std(l[slope == sl_])\n",
    "\n",
    "sig = lf[lf == np.min(lf)] - s\n",
    "sig2 = lf[lf == np.min(lf)] - s2\n",
    "sig3 = lf[lf == np.min(lf)] - s3\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20), nrows=2, ncols=1, gridspec_kw={'height_ratios': [4, 2.9]})\n",
    "ax[0].plot(slp, lf, linewidth=4, color='blue')\n",
    "ax[0].axhline(y=sig, linewidth=4, color='black', linestyle='dashed')\n",
    "ax[0].axhline(y=sig2, linewidth=4, color='black', linestyle='dashed')\n",
    "ax[0].axhline(y=sig3, linewidth=4, color='black', linestyle='dashed')\n",
    "ax[0].set_ylim([np.min(lf) - 1, np.min(lf) + 10])\n",
    "ax[0].set_ylabel('$ - \\ln \\mathcal{L}(F)$', fontsize=30.0)\n",
    "\n",
    "gam=np.zeros(len(slope))\n",
    "\n",
    "length=len(slp)\n",
    "\n",
    "llf=l.reshape(-1,length)\n",
    "s=slope.reshape(-1,length)\n",
    "\n",
    "for i in range (0,len(slope),length):\n",
    "    gam[i]=slope[i:i+length][l[i:i+length]==np.min(l[i:i+length])]\n",
    "    \n",
    "gam=gam[gam!=0]\n",
    "\n",
    "# calculate percentiles\n",
    "percentiles = [0.16, 0.84, 0.025, 0.975]\n",
    "quantiles = np.quantile(gam, percentiles)\n",
    "\n",
    "bins = slp\n",
    "\n",
    "# fit normal distribution to data\n",
    "(mu, sigma) = norm.fit(gam)\n",
    "\n",
    "# calculate PDF using fitted parameters\n",
    "pdf = norm.pdf(bins, mu, sigma)\n",
    "\n",
    "\n",
    "# Rescale the PDF to match the height of the histogram\n",
    "hist, bins = np.histogram(gam, bins=bins)\n",
    "bin_width = np.diff(bins)[0]\n",
    "pdf_max = np.max(pdf)\n",
    "pdf_scale = np.max(hist) / pdf_max\n",
    "pdf = pdf * pdf_scale\n",
    "\n",
    "\n",
    "ax[1].hist(gam, bins, color='r', histtype='bar', ec='black')\n",
    "ax[1].set_xlabel(r'$\\gamma$', fontsize=30.0)\n",
    "ax[1].set_ylabel('Counts', fontsize=35)\n",
    "ax[1].plot(bins, pdf, color='black', lw=2)\n",
    "ax[1].axvline(quantiles[0],linewidth=9,color='black',linestyle='dotted')\n",
    "ax[1].axvline(quantiles[1],linewidth=9,color='black',linestyle='dotted')\n",
    "ax[1].axvline(quantiles[2],linewidth=9,color='black',linestyle='dotted')\n",
    "ax[1].axvline(quantiles[3],linewidth=9,color='black',linestyle='dotted')\n",
    "fig.tight_layout()\n",
    "plt.savefig('pl_loop.pdf')\n",
    "plt.show()\n",
    "\n",
    "#________________ LOG NOISE LEVEL ___________________________\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=18) \n",
    "matplotlib.rc('ytick', labelsize=25) \n",
    "plt.rc('font', size=40)\n",
    "\n",
    "\n",
    "data = np.loadtxt('pl_log_noise.dat')\n",
    "slp = data[:, 0]\n",
    "\n",
    "l = [data[:, i] for i in range(1, 11)]\n",
    "\n",
    "g1 = np.arange(1.5, 2.5, 0.025) \n",
    "\n",
    "lf = np.zeros((10, len(g1)))\n",
    "\n",
    "for i, g_ in enumerate(g1):\n",
    "    lf[:, i] = [np.mean(l[j][slp == g_]) for j in range(10)]\n",
    "\n",
    "length = len(g1)\n",
    "slp_indices = np.arange(0, len(slp), length)\n",
    "slp_reshape = slp.reshape(-1, length)\n",
    "l_reshape = [l[j].reshape(-1, length) for j in range(10)]\n",
    "min_indices = [np.argmin(l, axis=1) for l in l_reshape]\n",
    "selected_indices = [np.random.choice(min_idx, size=len(slp_idx), replace=False) for min_idx, slp_idx in zip(min_indices, [slp_indices] * 10)]\n",
    "gamma = [g_rsh[sel_idx] for g_rsh, sel_idx in zip(slp_reshape, selected_indices)]\n",
    "\n",
    "slope=2.0 # true  slope\n",
    "\n",
    "x=np.array([0.5, 0.57009, 0.64565, 0.72654, 0.81283, 0.90485, 1.002, 1.1049, 1.2144, 2])\n",
    "y=np.mean(gamma,axis=1)/slope # ratio of the fitted slope / true slope \n",
    "y_err=(1/slope)*np.std(gamma,axis=1) # error bars on the fitted slope\n",
    "\n",
    "# Overplotting the results from a single simulation\n",
    "\n",
    "# Convert the 'gamma' lists into NumPy arrays\n",
    "\n",
    "gamma = np.array(gamma)\n",
    "\n",
    "# Calculate the mean of each row\n",
    "row_means = np.mean(gamma, axis=1)\n",
    "\n",
    "# Calculate the differences between each element and the mean for each row\n",
    "diffs = np.abs(gamma - row_means[:, np.newaxis])\n",
    "\n",
    "# Calculate the combined differences for each index\n",
    "combined_diffs = np.sum(diffs, axis=0)\n",
    "\n",
    "# Find the index closest to the mean for each row\n",
    "closest_indices = np.abs(gamma - row_means[:, np.newaxis]).argmin(axis=1)\n",
    "\n",
    "# Use the closest_indices to extract the corresponding values from each row\n",
    "\n",
    "single_sim_slp = (1/slope) * gamma[np.arange(gamma.shape[0]), closest_indices]\n",
    "\n",
    "\n",
    "# Plot configuration\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(x, y, marker='o', color='blue', linewidth=4, label='Mean of 100 simulations')\n",
    "plt.fill_between(x, y-y_err, y+y_err, color='red', alpha=0.4, label=r'$1-\\sigma$ deviation of 100 simulations')\n",
    "plt.plot(x, single_sim_slp, marker='d', color='k', linewidth=4, label='Single Simulation')\n",
    "\n",
    "# Set the axis labels and scale\n",
    "plt.xlabel(r'Relative noise level in units of $\\sigma_{F}$')\n",
    "plt.xscale('log')\n",
    "plt.ylabel(r'$\\frac{\\gamma_{best-fit} }{ \\gamma_{true}}$')\n",
    "\n",
    "# Set the grid and legend\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best', fontsize=22)\n",
    "\n",
    "# Set the x-axis tick values and labels\n",
    "plt.xticks([0.5, 0.81283, 1.002,  1.2144, 2], [0.5, 0.81283, 1.002,  1.2144, 2])\n",
    "\n",
    "# Save and display the plot\n",
    "plt.savefig(\"pl_noise.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb9e8e",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Verdana', cursive, sans-serif; font-weight: bold; color: green;font-size: 38px;\">For The Broken Power law PSD Model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c5d87",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Palatino', cursive, sans-serif; font-weight: bold; color: blue;font-size: 30px;\">Simple Case</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "n_realizations = 100\n",
    "results_list = []\n",
    "\n",
    "for i in range(n_realizations):\n",
    "    \n",
    "    np.random.seed(i)\n",
    "\n",
    "# broken power law parameters\n",
    "\n",
    "    gamma1=1.\n",
    "    gamma2=3\n",
    "    w0=0.3\n",
    "\n",
    "    alpha1=1.\n",
    "\n",
    "\n",
    "    N=600 # number of points in simulated light curve\n",
    "    step=1 # time sampling of simulated light curve\n",
    "\n",
    "    f_dc=5\n",
    "\n",
    "#noise term\n",
    "\n",
    "    noise_level=0.03\n",
    "\n",
    "    sigma_F=f_dc*noise_level\n",
    "\n",
    "#Fourier frequencies\n",
    "\n",
    "    w=2*(np.pi)*np.linspace(1,N,N)/N/step\n",
    "\n",
    "#Power at each frequency depending on power spectrum model. \n",
    "\n",
    "\n",
    "    def power (gamma1,gamma2,w0):\n",
    "        \n",
    "        p = np.where(w <= w0, (w / w0)**(-gamma1), (w / w0)**(-gamma2))\n",
    "                \n",
    "        return p\n",
    "\n",
    "\n",
    "#Generate two sets of normally distributed random numbers \n",
    "    \n",
    "    aa = np.random.randn(len(w))\n",
    "    bb = np.random.randn(len(w))\n",
    "    \n",
    "# Light curve in frequency domain  = SQRT(p/2) * (a + b*i)\n",
    "\n",
    "    f_w = np.sqrt(0.5*power(gamma1,gamma2,w0))*(aa + bb*1j)\n",
    "    f_w[0]=f_dc * len(w)**0.5\n",
    "\n",
    "#generating noise\n",
    "\n",
    "    n=np.ones(len(w))*(sigma_F**2)\n",
    "    n_w=np.sqrt(0.5*n)*(aa + bb*1.j)\n",
    "    n_w[0] = np.random.randn(1)*sigma_F\n",
    "\n",
    "# Generating the total flux (Signal + noise)\n",
    "\n",
    "    f1_w = alpha1 * f_w\n",
    "    phi_w = f1_w \n",
    "    F_w = phi_w+n_w\n",
    "\n",
    "    f_t = np.real(np.fft.ifft(f_w[:], norm='ortho'))\n",
    "    Ft = np.real(np.fft.ifft(F_w[:], norm='ortho'))\n",
    "\n",
    "\n",
    "    def Sigma_phi(alpha1, gamma1,gamma2,w0):\n",
    "        Sigma_phi_ = (alpha1**2) * power(gamma1,gamma2,w0)\n",
    "        return Sigma_phi_\n",
    "\n",
    "\n",
    "    def cov_mat(alpha1,gamma1,gamma2,w0):\n",
    "        dt = np.linspace(0, N, N)\n",
    "        cov=Sigma_phi(alpha1, gamma1,gamma2,w0)\n",
    "\n",
    "# calculate sigma for all dt values\n",
    "        sigma = 2 * np.trapz(cov * np.cos(w * dt[:, np.newaxis]), w, axis=1)\n",
    "\n",
    "# add noise to the first element of sigma\n",
    "        sigma[0] += sigma_F**2\n",
    "        \n",
    "# create the covariance matrix by subtracting the absolute difference between indices\n",
    "    \n",
    "        ID = np.arange(sigma.size)\n",
    "        sigma = sigma[np.abs(ID - ID[:, None])]\n",
    "\n",
    "        return sigma\n",
    "\n",
    "    \n",
    "    def log_like_F (alpha1, gamma1,gamma2,w0):\n",
    "    \n",
    "        mu=np.mean(Ft)\n",
    "      \n",
    "        data=Ft-mu\n",
    "    \n",
    "        inv=np.linalg.inv(cov_mat(alpha1,gamma1,gamma2,w0))\n",
    "    \n",
    "        sign,logdet=np.linalg.slogdet(2*np.pi*cov_mat( alpha1, gamma1,gamma2,w0))\n",
    "    \n",
    "        lh=-0.5*( logdet + np.transpose(data) @ inv @ data )\n",
    "    \n",
    "        return lh\n",
    "\n",
    "\n",
    "\n",
    "    def mnz(args):\n",
    "        alpha1, gamma1, gamma2, w0 = args\n",
    "        return -log_like_F(alpha1, gamma1,gamma2,w0)+log_like_F(1,1,1,1)\n",
    "\n",
    "\n",
    "\n",
    "    w_max=0.7\n",
    "    w_min=0.1\n",
    "    w_step=0.001\n",
    "\n",
    "    ww=np.arange(w_min,w_max,w_step)  \n",
    "\n",
    "    diffe=np.zeros(len(ww))\n",
    "    g=np.zeros(len(ww))\n",
    "    amp=np.zeros(len(ww))\n",
    "\n",
    "    results = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.4,1,3,w_), bounds=((0.1,0.9),(1,1),(1.5,5),(w_,w_)))\n",
    "        for w_ in ww\n",
    "    )\n",
    "\n",
    "    for j, res1 in enumerate(results):\n",
    "        amp[j] = res1.x[0]\n",
    "        g[j]=float(res1.x[2])\n",
    "        diffe[j] = -log_like_F(amp[j],1,g[j],ww[j])+log_like_F(1,1,1,1)\n",
    "\n",
    "    results_list.append({'amp': amp,'g': g, 'diffe': diffe})\n",
    "\n",
    "    rows=zip(np.round(ww,3),np.round(amp,3),np.round(g,2),np.round(diffe,2))\n",
    "\n",
    "    with open ('bpl.dat', 'a') as f:\n",
    "        writer=csv.writer(f, delimiter=' ')\n",
    "        for i in rows:\n",
    "            writer.writerow(i)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ba9ac",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Palatino', cursive, sans-serif; font-weight: bold; color: blue;font-size: 30px;\">LC analysis in Segments</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "n_realizations = 100\n",
    "results_list = []\n",
    "\n",
    "for i in range(n_realizations):\n",
    "    \n",
    "    np.random.seed(i)\n",
    "\n",
    "    gamma1=1.\n",
    "    gamma2=3\n",
    "    w0=0.3\n",
    "\n",
    "    alpha1=1.\n",
    "\n",
    "\n",
    "    N=600 # number of points in simulated light curve\n",
    "    step=1 # time sampling of simulated light curve\n",
    "\n",
    "    f_dc=5\n",
    "\n",
    "#noise term\n",
    "\n",
    "    noise_level=0.03\n",
    "\n",
    "    sigma_F=f_dc*noise_level\n",
    "\n",
    "#Fourier frequencies\n",
    "\n",
    "    w=2*(np.pi)*np.linspace(1,N,N)/N/step\n",
    "\n",
    "#Power at each frequency depending on power spectrum model. \n",
    "\n",
    "\n",
    "    def power (gamma1,gamma2,w0):\n",
    "    \n",
    "        p = np.where(w <= w0, (w / w0)**(-gamma1), (w / w0)**(-gamma2))\n",
    "                \n",
    "        return p\n",
    "\n",
    "\n",
    "#Generate two sets of normally distributed random numbers \n",
    "    \n",
    "    aa = np.random.randn(len(w))\n",
    "    bb = np.random.randn(len(w))\n",
    "    \n",
    "# Light curve in frequency domain  = SQRT(p/2) * (a + b*i)\n",
    "\n",
    "    f_w = np.sqrt(0.5*power(gamma1,gamma2,w0))*(aa + bb*1j)\n",
    "    f_w[0]=f_dc * len(w)**0.5\n",
    "\n",
    "#generating noise\n",
    "\n",
    "    n=np.ones(len(w))*(sigma_F**2)\n",
    "    n_w=np.sqrt(0.5*n)*(aa + bb*1.j)\n",
    "    n_w[0] = np.random.randn(1)*sigma_F\n",
    "\n",
    "# Generating the total flux (Signal + noise)\n",
    "\n",
    "    f1_w = alpha1 * f_w\n",
    "    phi_w = f1_w \n",
    "    F_w = phi_w+n_w\n",
    "\n",
    "    f_t = np.real(np.fft.ifft(f_w[:], norm='ortho'))\n",
    "    F_t = np.real(np.fft.ifft(F_w[:], norm='ortho'))\n",
    "\n",
    "    lim=200\n",
    "    low=2\n",
    "    high=3\n",
    "\n",
    "    Ft=F_t[low*lim:high*lim]\n",
    "\n",
    "    def Sigma_phi(alpha1, gamma1,gamma2,w0):\n",
    "        Sigma_phi_ = (alpha1**2) * power(gamma1,gamma2,w0)\n",
    "        return Sigma_phi_\n",
    "\n",
    "\n",
    "    def cov_mat(alpha1,gamma1,gamma2,w0):\n",
    "    \n",
    "        dt=np.linspace(0,lim,lim)    \n",
    "        cov=Sigma_phi(alpha1, gamma1,gamma2,w0)\n",
    "    \n",
    "        sigma = 2 * np.trapz(cov * np.cos(w * dt[:, np.newaxis]), w, axis=1)\n",
    "           \n",
    "# adding noise to the covariance matrix     \n",
    "    \n",
    "        sigma[0]+=sigma_F**2   \n",
    "        ID = np.arange(sigma.size)\n",
    "        sigma= sigma[np.abs(ID - ID[:,None])]\n",
    "    \n",
    "        return sigma\n",
    "    \n",
    "    def log_like_F (alpha1, gamma1,gamma2,w0):\n",
    "    \n",
    "        mu=np.mean(Ft)\n",
    "      \n",
    "        data=Ft-mu\n",
    "    \n",
    "        inv=np.linalg.inv(cov_mat(alpha1,gamma1,gamma2,w0))\n",
    "    \n",
    "        sign,logdet=np.linalg.slogdet(2*np.pi*cov_mat( alpha1, gamma1,gamma2,w0))\n",
    "    \n",
    "        lh=-0.5*( logdet + np.transpose(data) @ inv @ data )\n",
    "    \n",
    "        return lh\n",
    "\n",
    "\n",
    "    def mnz(args):\n",
    "            alpha1, gamma1, gamma2, w0 = args\n",
    "            return -log_like_F(alpha1, gamma1,gamma2,w0)+log_like_F(1,1,1,1)\n",
    "\n",
    "\n",
    "\n",
    "    w_max=0.7\n",
    "    w_min=0.1\n",
    "    w_step=0.001\n",
    "\n",
    "    ww=np.arange(w_min,w_max,w_step)  \n",
    "\n",
    "    diffe=np.zeros(len(ww))\n",
    "    g=np.zeros(len(ww))\n",
    "    amp=np.zeros(len(ww))\n",
    "\n",
    "    results = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.4,1,3,w_), bounds=((0.05,0.9),(1,1),(1.5,5),(w_,w_)))\n",
    "        for w_ in ww\n",
    "    )\n",
    "\n",
    "    for j, res1 in enumerate(results):\n",
    "        amp[j] = res1.x[0]\n",
    "        g[j]=float(res1.x[2])\n",
    "        diffe[j] = -log_like_F(amp[j],1,g[j],ww[j])+log_like_F(1,1,1,1)\n",
    "\n",
    "    results_list.append({'amp': amp,'g': g, 'diffe': diffe})\n",
    "\n",
    "    rows=zip(np.round(ww,3),np.round(amp,3),np.round(g,2),np.round(diffe,2))\n",
    "\n",
    " #   with open ('b1.dat', 'a') as f:\n",
    " #   with open ('b2.dat', 'a') as f:\n",
    "    with open ('b3.dat', 'a') as f:\n",
    "        writer=csv.writer(f, delimiter=' ')\n",
    "        for i in rows:\n",
    "            writer.writerow(i)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e4c2f",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Palatino', cursive, sans-serif; font-weight: bold; color: blue;font-size: 30px;\">Effect of Gaps and Interpolation Schemes</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "n_realizations = 100\n",
    "results_list = []\n",
    "\n",
    "for i in range(n_realizations):\n",
    "    \n",
    "    np.random.seed(i)\n",
    "\n",
    "    gamma1=1.\n",
    "    gamma2=3\n",
    "    w0=0.3\n",
    "\n",
    "    alpha1=1.\n",
    "\n",
    "    \n",
    "    N=600 # number of points in simulated light curve\n",
    "    step=1 # time sampling of simulated light curve\n",
    "    \n",
    "    f_dc=0.1\n",
    "    \n",
    "    #noise term\n",
    "    \n",
    "    noise_level=0.03\n",
    "    \n",
    "    sigma_F=f_dc*noise_level\n",
    "    \n",
    "    #Fourier frequencies\n",
    "    \n",
    "    w=2*(np.pi)*np.linspace(1,N,N)/N/step\n",
    "    \n",
    "    #Power at each frequency depending on power spectrum model. \n",
    "    \n",
    "    \n",
    "    def power (gamma1,gamma2,w0):\n",
    "    \n",
    "        p = np.where(w <= w0, (w / w0)**(-gamma1), (w / w0)**(-gamma2))\n",
    "                \n",
    "        return p\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Generate two sets of normally distributed random numbers \n",
    "        \n",
    "    aa = np.random.randn(len(w))\n",
    "    bb = np.random.randn(len(w))\n",
    "        \n",
    "    # Light curve in frequency domain  = SQRT(p/2) * (a + b*i)\n",
    "    \n",
    "    f_w = np.sqrt(0.5*power(gamma1,gamma2,w0))*(aa + bb*1j)\n",
    "    \n",
    "    f_w[0]=f_dc * len(w)**0.5\n",
    "    \n",
    "    #generating noise\n",
    "    \n",
    "    n=np.ones(len(w))*(sigma_F**2)\n",
    "    \n",
    "    n_w=np.sqrt(0.5*n)*(aa + bb*1.j)\n",
    "    \n",
    "    n_w[0] = np.random.randn(1)*sigma_F\n",
    "    \n",
    "    # Generating the total flux (Signal + noise)\n",
    "    \n",
    "    f1_w = alpha1 * f_w\n",
    "    phi_w = f1_w \n",
    "    F_w = phi_w+n_w\n",
    "    \n",
    "    f_t = np.real(np.fft.ifft(f_w[:], norm='ortho'))\n",
    "    \n",
    "    F_t = np.real(np.fft.ifft(F_w[:], norm='ortho'))\n",
    "\n",
    "# section to remove points and interpolation\n",
    "\n",
    "    T=np.linspace(0,N,N)\n",
    "\n",
    "    # Calculate the number of points to remove\n",
    "    num_points = int(len(T) * 0.1)\n",
    "\n",
    "    # Randomly select indices to remove with gaps ranging from 1 to 10 points\n",
    "    remove_indices = []\n",
    "    num_removed = 0\n",
    "    while num_removed < num_points:\n",
    "        index = np.random.randint(0, len(T) - 10)\n",
    "        gap = np.random.randint(1, 11)\n",
    "        remove_indices.extend(range(index, min(index + gap, len(T))))\n",
    "        num_removed += gap\n",
    "\n",
    "    # Remove the selected data points from the time series and the flux array\n",
    "    t4 = np.delete(T, remove_indices)\n",
    "    Ft4 = np.delete(F_t, remove_indices)\n",
    "\n",
    "#    interpolation schemes\n",
    "\n",
    "   # f = interpolate.interp1d(t4,Ft4,kind='linear',fill_value='extrapolate')\n",
    "  #  f = Rbf(t4,Ft4,function='cubic',fill_value=1e+10)\n",
    " \n",
    "\n",
    " # # This section is for the Gaussian Process Interpolation \n",
    "  \n",
    " # Define the Kriging model with Matern kernel\n",
    "    \n",
    "    kernel = Matern(length_scale=0.2, nu=1.5)\n",
    "    model = GaussianProcessRegressor(kernel=kernel)\n",
    "     \n",
    "     # Train the model\n",
    "     \n",
    "    model.fit(t4.reshape(-1, 1), Ft4)\n",
    "    Ft=model.predict(T.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    def Sigma_phi(alpha1, gamma1,gamma2,w0):\n",
    "            Sigma_phi_ = (alpha1**2) * power(gamma1,gamma2,w0)\n",
    "            return Sigma_phi_\n",
    "\n",
    "\n",
    "    def cov_mat(alpha1,gamma1,gamma2,w0):\n",
    "        dt = np.linspace(0, N, N)\n",
    "        cov=Sigma_phi(alpha1, gamma1,gamma2,w0)\n",
    "\n",
    "# calculate sigma for all dt values\n",
    "        sigma = 2 * np.trapz(cov * np.cos(w * dt[:, np.newaxis]), w, axis=1)\n",
    "\n",
    "# add noise to the first element of sigma\n",
    "        sigma[0] += sigma_F**2\n",
    "        \n",
    "# create the covariance matrix by subtracting the absolute difference between indices\n",
    "    \n",
    "        ID = np.arange(sigma.size)\n",
    "        sigma = sigma[np.abs(ID - ID[:, None])]\n",
    "\n",
    "        return sigma\n",
    "\n",
    "    \n",
    "    def log_like_F (alpha1, gamma1,gamma2,w0):\n",
    "    \n",
    "        mu=np.mean(Ft)\n",
    "      \n",
    "        data=Ft-mu\n",
    "    \n",
    "        inv=np.linalg.inv(cov_mat(alpha1,gamma1,gamma2,w0))\n",
    "    \n",
    "        sign,logdet=np.linalg.slogdet(2*np.pi*cov_mat( alpha1, gamma1,gamma2,w0))\n",
    "    \n",
    "        lh=-0.5*( logdet + np.transpose(data) @ inv @ data )\n",
    "    \n",
    "        return lh\n",
    "\n",
    "\n",
    "\n",
    "    def mnz(args):\n",
    "        alpha1, gamma1, gamma2, w0 = args\n",
    "        return -log_like_F(alpha1, gamma1,gamma2,w0)+log_like_F(1,1,1,1)\n",
    "\n",
    "\n",
    "\n",
    "    w_max=0.7\n",
    "    w_min=0.1\n",
    "    w_step=0.001\n",
    "\n",
    "    ww=np.arange(w_min,w_max,w_step)  \n",
    "\n",
    "    diffe=np.zeros(len(ww))\n",
    "    g=np.zeros(len(ww))\n",
    "    amp=np.zeros(len(ww))\n",
    "\n",
    "    results = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.4,1,3,w_), bounds=((0.005,5),(1,1),(1.5,10),(w_,w_)))\n",
    "        for w_ in ww\n",
    "    )\n",
    "\n",
    "    for j, res1 in enumerate(results):\n",
    "        amp[j] = res1.x[0]\n",
    "        g[j]=float(res1.x[2])\n",
    "        diffe[j] = -log_like_F(amp[j],1,g[j],ww[j])+log_like_F(1,1,1,1)\n",
    "\n",
    "    results_list.append({'amp': amp,'g': g, 'diffe': diffe})\n",
    "\n",
    "    rows=zip(np.round(ww,3),np.round(amp,3),np.round(g,2),np.round(diffe,2))\n",
    "\n",
    " #   with open ('bpl_lin.dat', 'a') as f:\n",
    " #   with open ('bpl_cub.dat', 'a') as f:\n",
    "    with open ('bpl_gas.dat', 'a') as f:\n",
    "        writer=csv.writer(f, delimiter=' ')\n",
    "        for i in rows:\n",
    "            writer.writerow(i)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a2f042",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Palatino', cursive, sans-serif; font-weight: bold; color: blue;font-size: 30px;\">Effect of Incorrect Noise Modeling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a7fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "n_realizations = 100\n",
    "\n",
    "\n",
    "for i in range(n_realizations):\n",
    "    \n",
    "    np.random.seed(i)\n",
    "\n",
    "# broken power law parameters\n",
    "\n",
    "    gamma1=1.\n",
    "    gamma2=3\n",
    "    w0=0.3\n",
    "\n",
    "    alpha1=1.\n",
    "\n",
    "\n",
    "    N=600 # number of points in simulated light curve\n",
    "    step=1 # time sampling of simulated light curve\n",
    "\n",
    "    f_dc=0.1\n",
    "\n",
    "#noise term\n",
    "\n",
    "    noise_level=0.03\n",
    "    sigma_F=f_dc*noise_level\n",
    "\n",
    "#Fourier frequencies\n",
    "\n",
    "    w=2*(np.pi)*np.linspace(1,N,N)/N/step\n",
    "\n",
    "#Power at each frequency depending on power spectrum model. \n",
    "\n",
    "\n",
    "    def power (gamma1,gamma2,w0):\n",
    "        \n",
    "        p = np.where(w <= w0, (w / w0)**(-gamma1), (w / w0)**(-gamma2))\n",
    "                \n",
    "        return p\n",
    "\n",
    "\n",
    "#Generate two sets of normally distributed random numbers \n",
    "    \n",
    "    aa = np.random.randn(len(w))\n",
    "    bb = np.random.randn(len(w))\n",
    "    \n",
    "# Light curve in frequency domain  = SQRT(p/2) * (a + b*i)\n",
    "\n",
    "    f_w = np.sqrt(0.5*power(gamma1,gamma2,w0))*(aa + bb*1j)\n",
    "    f_w[0]=f_dc * len(w)**0.5\n",
    "\n",
    "#generating noise\n",
    "\n",
    "    n=np.ones(len(w))*(sigma_F**2)\n",
    "    n_w=np.sqrt(0.5*n)*(aa + bb*1.j)\n",
    "    n_w[0] = np.random.randn(1)*sigma_F\n",
    "\n",
    "# Generating the total flux (Signal + noise)\n",
    "\n",
    "    f1_w = alpha1 * f_w\n",
    "    phi_w = f1_w \n",
    "    F_w = phi_w+n_w\n",
    "\n",
    "    f_t = np.real(np.fft.ifft(f_w[:], norm='ortho'))\n",
    "    F_t = np.real(np.fft.ifft(F_w[:], norm='ortho'))\n",
    "\n",
    "\n",
    "    def Sigma_phi(alpha1, gamma1,gamma2,w0):\n",
    "        Sigma_phi_ = (alpha1**2) * power(gamma1,gamma2,w0)\n",
    "        return Sigma_phi_\n",
    "    \n",
    "    \n",
    "    def cov_mat(alpha1,gamma1,gamma2,w0,sigma_F):\n",
    "        \n",
    "        dt = np.linspace(0, N, N)\n",
    "        cov=Sigma_phi(alpha1, gamma1,gamma2,w0)\n",
    "        \n",
    "        # calculate sigma for all dt values\n",
    "        sigma = 2 * np.trapz(cov * np.cos(w * dt[:, np.newaxis]), w, axis=1)\n",
    "    \n",
    "        # add noise to the first element of sigma\n",
    "        sigma[0] += sigma_F**2\n",
    "                \n",
    "        # create the covariance matrix by subtracting the absolute difference between indices\n",
    "            \n",
    "        ID = np.arange(sigma.size)\n",
    "        sigma = sigma[np.abs(ID - ID[:, None])]\n",
    "    \n",
    "        return sigma\n",
    "    \n",
    "    \n",
    "        \n",
    "    def log_like_F (alpha1, gamma1,gamma2,w0,sigma_F):\n",
    "        \n",
    "        mu=np.mean(F_t)\n",
    "          \n",
    "        data=F_t-mu\n",
    "        \n",
    "        inv=np.linalg.inv(cov_mat(alpha1,gamma1,gamma2,w0,sigma_F))\n",
    "        \n",
    "        sign,logdet=np.linalg.slogdet(2*np.pi*cov_mat( alpha1, gamma1,gamma2,w0,sigma_F))\n",
    "        \n",
    "        lh=-0.5*( logdet + np.transpose(data) @ inv @ data )\n",
    "        \n",
    "        return lh\n",
    "    \n",
    "    \n",
    "    \n",
    "    def mnz(args):\n",
    "            alpha1, gamma1, gamma2, w0, sigma_F = args\n",
    "            return -log_like_F(alpha1, gamma1,gamma2,w0,sigma_F)+log_like_F(1,1,1,1,sigma_F)\n",
    "    \n",
    "    \n",
    "    \n",
    "    w_max=0.7\n",
    "    w_min=0.1\n",
    "    w_step=0.001\n",
    "    \n",
    "    ww=np.arange(w_min,w_max,w_step)  \n",
    "    \n",
    "    # Noise level from 0.5 to 2.0 in 5 log steps\n",
    "    \n",
    "    ss1, ss2, ss3, ss4, ss5, ss6, ss7, ss8, ss9, ss10 = np.array([0.5, 0.57009, 0.64565, 0.72654, 0.81283, 0.90485, 1.002, 1.1049, 1.2144, 2]) * sigma_F\n",
    "    diffe1, diffe2, diffe3, diffe4, diffe5, diffe6, diffe7, diffe8, diffe9, diffe10  = np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww))\n",
    "    gg1, gg2, gg3, gg4, gg5, gg6, gg7, gg8, gg9, gg10 = np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww))\n",
    "    amp1, amp2, amp3, amp4, amp5, amp6, amp7, amp8, amp9, amp10 = np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww)), np.zeros(len(ww))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    results = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.4,1,3,w_,ss1), bounds=((0.05,5),(1,1),(1.,10),(w_,w_),(ss1,ss1)))\n",
    "        for w_ in ww\n",
    "    )\n",
    "    \n",
    "    for j, res1 in enumerate(results):\n",
    "        amp1[j] = res1.x[0]\n",
    "        gg1[j]=float(res1.x[2])\n",
    "        diffe1[j] = -log_like_F(amp1[j],1,gg1[j],ww[j],ss1)+log_like_F(1,1,1,1,ss1)\n",
    "        \n",
    "    results2 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.4,1,3,w_,ss2), bounds=((0.05,5),(1,1),(1.,10),(w_,w_),(ss2,ss2)))\n",
    "        for w_ in ww\n",
    "    )\n",
    "    \n",
    "    for j, res2 in enumerate(results2):\n",
    "        amp2[j] = res2.x[0]\n",
    "        gg2[j]=float(res2.x[2])\n",
    "        diffe2[j] = -log_like_F(amp2[j],1,gg2[j],ww[j],ss2)+log_like_F(1,1,1,1,ss2)\n",
    "        \n",
    "    results3 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.4,1,3,w_,ss3), bounds=((0.05,5),(1,1),(1.,10),(w_,w_),(ss3,ss3)))\n",
    "        for w_ in ww\n",
    "    )\n",
    "    \n",
    "    for j, res3 in enumerate(results3):\n",
    "        amp3[j] = res3.x[0]\n",
    "        gg3[j]=float(res3.x[2])\n",
    "        diffe3[j] = -log_like_F(amp3[j],1,gg3[j],ww[j],ss3)+log_like_F(1,1,1,1,ss3)\n",
    "        \n",
    "        \n",
    "    results4 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.4,1,3,w_,ss4), bounds=((0.005,150),(1,1),(1.,10),(w_,w_),(ss4,ss4)))\n",
    "        for w_ in ww\n",
    "    )\n",
    "    \n",
    "    for j, res4 in enumerate(results4):\n",
    "        amp4[j] = res4.x[0]\n",
    "        gg4[j]=float(res4.x[2])\n",
    "        diffe4[j] = -log_like_F(amp4[j],1,gg4[j],ww[j],ss4)+log_like_F(1,1,1,1,ss4)\n",
    "        \n",
    "        \n",
    "    results5 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.4,1,3,w_,ss5), bounds=((0.005,150),(1,1),(1.,10),(w_,w_),(ss5,ss5)))\n",
    "        for w_ in ww\n",
    "    )\n",
    "    \n",
    "    for j, res5 in enumerate(results5):\n",
    "        amp5[j] = res5.x[0]\n",
    "        gg5[j]=float(res5.x[2])\n",
    "        diffe5[j] = -log_like_F(amp5[j],1,gg5[j],ww[j],ss5)+log_like_F(1,1,1,1,ss5)\n",
    "        \n",
    "        \n",
    "    results6 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.4,1,3,w_,ss6), bounds=((0.05,5),(1,1),(1.,10),(w_,w_),(ss6,ss6)))\n",
    "        for w_ in ww\n",
    "    )\n",
    "    \n",
    "    for j, res6 in enumerate(results6):\n",
    "        amp6[j] = res6.x[0]\n",
    "        gg6[j]=float(res6.x[2])\n",
    "        diffe6[j] = -log_like_F(amp6[j],1,gg6[j],ww[j],ss6)+log_like_F(1,1,1,1,ss6)\n",
    "        \n",
    "    results7 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.4,1,3,w_,ss7), bounds=((0.05,5),(1,1),(1.,10),(w_,w_),(ss7,ss7)))\n",
    "        for w_ in ww\n",
    "    )\n",
    "    \n",
    "    for j, res7 in enumerate(results7):\n",
    "        amp7[j] = res7.x[0]\n",
    "        gg7[j]=float(res7.x[2])\n",
    "        diffe7[j] = -log_like_F(amp7[j],1,gg7[j],ww[j],ss7)+log_like_F(1,1,1,1,ss7)\n",
    "        \n",
    "    results8 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.4,1,3,w_,ss8), bounds=((0.05,5),(1,1),(1.,10),(w_,w_),(ss8,ss8)))\n",
    "        for w_ in ww\n",
    "    )\n",
    "    \n",
    "    for j, res8 in enumerate(results8):\n",
    "        amp8[j] = res8.x[0]\n",
    "        gg8[j]=float(res8.x[2])\n",
    "        diffe8[j] = -log_like_F(amp8[j],1,gg8[j],ww[j],ss8)+log_like_F(1,1,1,1,ss8)\n",
    "        \n",
    "        \n",
    "    results9 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.4,1,3,w_,ss9), bounds=((0.005,150),(1,1),(1.,10),(w_,w_),(ss9,ss9)))\n",
    "        for w_ in ww\n",
    "    )\n",
    "    \n",
    "    for j, res9 in enumerate(results9):\n",
    "        amp9[j] = res9.x[0]\n",
    "        gg9[j]=float(res9.x[2])\n",
    "        diffe9[j] = -log_like_F(amp9[j],1,gg9[j],ww[j],ss9)+log_like_F(1,1,1,1,ss9)\n",
    "        \n",
    "        \n",
    "    results10 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(minimize)(mnz, (0.4,1,3,w_,ss10), bounds=((0.005,150),(1,1),(1.,10),(w_,w_),(ss10,ss10)))\n",
    "        for w_ in ww\n",
    "    )\n",
    "    \n",
    "    for j, res10 in enumerate(results10):\n",
    "        amp10[j] = res10.x[0]\n",
    "        gg10[j]=float(res10.x[2])\n",
    "        diffe10[j] = -log_like_F(amp10[j],1,gg10[j],ww[j],ss10)+log_like_F(1,1,1,1,ss10)\n",
    "    \n",
    "    \n",
    "    rows=zip(np.round(ww,3),np.round(gg1,2),np.round(diffe1,2),np.round(gg2,2),\n",
    "              np.round(diffe2,2),np.round(gg3,2),np.round(diffe3,2),np.round(gg4,2),\n",
    "                      np.round(diffe4,2),np.round(gg5,2),np.round(diffe5,2),\n",
    "                      np.round(gg6,2),np.round(diffe6,2),np.round(gg7,2),\n",
    "                                np.round(diffe7,2),np.round(gg8,2),np.round(diffe8,2),np.round(gg9,2),\n",
    "                                        np.round(diffe9,2),np.round(gg10,2),np.round(diffe10,2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open ('bpl_dif_noise.dat', 'a') as f:\n",
    "        writer=csv.writer(f, delimiter=' ')\n",
    "        for i in rows:\n",
    "               writer.writerow(i)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9ac55",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Century Gothic', cursive, sans-serif; font-weight: bold; color: purple;font-size: 30px;\">Template for Plots</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4782159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#______________ BROKEN POWER LAW LOOP __________________________\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=35) \n",
    "matplotlib.rc('ytick', labelsize=35) \n",
    "plt.rc('font', size=45)\n",
    "\n",
    "\n",
    "df = 3\n",
    "s = -0.5 * chi2.ppf(0.68, df)\n",
    "s2 = -0.5 * chi2.ppf(0.95, df)\n",
    "s3 = -0.5 * chi2.ppf(0.997, df)\n",
    "\n",
    "wbk=np.loadtxt('bpl.dat')[:,0]\n",
    "\n",
    "g=np.loadtxt('bpl.dat')[:,2]\n",
    "l=np.loadtxt('bpl.dat')[:,3]\n",
    "\n",
    "w_max=0.7\n",
    "w_min=0.1\n",
    "w_step=0.001\n",
    "ww=np.round(np.arange(w_min,w_max,w_step),3) \n",
    "\n",
    "lf = np.zeros(len(ww))\n",
    "gg = np.zeros(len(ww))\n",
    "\n",
    "\n",
    "\n",
    "for i, ww_ in enumerate(ww):\n",
    "    lf[i] = np.mean(l[wbk == ww_])\n",
    "    gg[i] = np.mean(g[wbk == ww_])\n",
    "    \n",
    "\n",
    "sig1=lf[lf==np.min(lf)]-s\n",
    "sig2=lf[lf==np.min(lf)]-s2\n",
    "sig3=lf[lf==np.min(lf)]-s3\n",
    "\n",
    "\n",
    "# For the \\omega bk Plot:-\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax.plot(ww, lf, linewidth=4, color='blue')\n",
    "ax.axhline(y=sig1, linewidth=4, color='black', linestyle='dashed')\n",
    "ax.axhline(y=sig2, linewidth=4, color='black', linestyle='dashed')\n",
    "ax.axhline(y=sig3, linewidth=4, color='black', linestyle='dashed')\n",
    "ax.set_ylim([np.min(lf) - 1, np.min(lf) + 10])\n",
    "ax.set_ylabel('$ - \\ln \\mathcal{L}(F)$')\n",
    "ax.set_xlabel('$\\omega_{bk}$ (rad/day)')\n",
    "plt.savefig('bpl_loop_wbk.pdf')\n",
    "plt.show()\n",
    "\n",
    "# For the \\gamma2 Plot:-\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax.plot(gg, lf, linewidth=4, color='blue')\n",
    "ax.axhline(y=sig1, linewidth=4, color='black', linestyle='dashed')\n",
    "ax.axhline(y=sig2, linewidth=4, color='black', linestyle='dashed')\n",
    "ax.axhline(y=sig3, linewidth=4, color='black', linestyle='dashed')\n",
    "ax.set_ylim([np.min(lf) - 1, np.min(lf) + 10])\n",
    "ax.set_ylabel('$ - \\ln \\mathcal{L}(F)$')\n",
    "ax.set_xlabel('$\\gamma_{2}$')\n",
    "plt.savefig('bpl_loop_slope.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=45) \n",
    "matplotlib.rc('ytick', labelsize=45) \n",
    "plt.rc('font', size=45)\n",
    "\n",
    "# For the Contour Plot:  \n",
    "    \n",
    "length=len(ww)\n",
    "    \n",
    "brk_indices = np.arange(0, len(wbk), length)\n",
    "wbk_reshape = wbk.reshape(-1, length)\n",
    "l_reshape = l.reshape(-1, length)\n",
    "min_indices = np.argmin(l_reshape, axis=1)\n",
    "selected_indices = np.random.choice(min_indices, size=len(brk_indices))\n",
    "brk = wbk_reshape[np.arange(len(brk_indices)), selected_indices]  \n",
    "\n",
    "gf_indices = np.arange(0, len(wbk), length)\n",
    "gf=g.reshape(-1,length)\n",
    "g2=gf[np.arange(len(brk_indices)), selected_indices] \n",
    "\n",
    "\n",
    "# Define confidence levels\n",
    "df = 2\n",
    "s = 0.5 * chi2.ppf(0.68, df)\n",
    "s2 = 0.5 * chi2.ppf(0.95, df)\n",
    "s3 = 0.5 * chi2.ppf(0.997, df)\n",
    "\n",
    "Level=[s,s2,s3]\n",
    "\n",
    "# Calculate mean and covariance matrix\n",
    "data = np.column_stack((brk, g2))\n",
    "mean=(np.mean(data,axis=0))\n",
    "cov = np.cov(data, rowvar=False)\n",
    "\n",
    "\n",
    "# # Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# # Create colorbar\n",
    "cmap = plt.get_cmap('Set1')\n",
    "vmin = lf.min()\n",
    "vmax = lf.max()\n",
    "norm = mcolors.TwoSlopeNorm(vmax=vmax, vcenter=(vmax+vmin)//2, vmin=vmin)\n",
    "sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "sm.set_array([])\n",
    "\n",
    "for i, conf_level in enumerate(Level):\n",
    "    lambda_, v = np.linalg.eig(cov)\n",
    "    lambda_ = np.sqrt(lambda_)\n",
    "    ell1 = Ellipse(xy=mean, width=lambda_[0]*np.sqrt(Level[0])*2,\n",
    "                  height=lambda_[1]*np.sqrt(Level[0])*2,\n",
    "                  angle=np.rad2deg(np.arccos(v[0, 0])), fill=True, alpha=1., facecolor=cmap(0))\n",
    "    ell2 = Ellipse(xy=mean, width=lambda_[0]*np.sqrt(Level[1])*2,\n",
    "                  height=lambda_[1]*np.sqrt(Level[1])*2,\n",
    "                  angle=np.rad2deg(np.arccos(v[0, 0])), fill=True, alpha=0.2, facecolor=cmap(1))\n",
    "    ell3 = Ellipse(xy=mean, width=lambda_[0]*np.sqrt(Level[2])*2,\n",
    "                  height=lambda_[1]*np.sqrt(Level[2])*2,\n",
    "                  angle=np.rad2deg(np.arccos(v[0, 0])), fill=True, alpha=0.2, facecolor=cmap(9))\n",
    "    \n",
    "    \n",
    "\n",
    "    ax.add_artist(ell1)\n",
    "    ax.add_artist(ell2)\n",
    "    ax.add_artist(ell3)\n",
    "\n",
    "\n",
    "# Set plot limits\n",
    "ax.set_xlim(0.1,0.56)\n",
    "ax.set_ylim(1.5,4.2)\n",
    "\n",
    "# Set background color\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(sm)\n",
    "\n",
    "# Set xticks and yticks\n",
    "ax.set_xlabel('$\\omega_{bk}$ (rad/day)')\n",
    "ax.set_ylabel('$\\gamma_{2}$')\n",
    "cbar.set_label('$- \\ln \\mathcal{L}(F)$')\n",
    "\n",
    "# Annotate a point with a marker and label\n",
    "point_x = 0.3\n",
    "point_y = 3.\n",
    "ax.plot(point_x, point_y, marker='*', markersize=35, color='k')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "fig.savefig('bpl_contour.pdf')\n",
    "\n",
    "\n",
    "#_______________ BROKEN POWER LAW SEGMENTS_____________________\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=25) \n",
    "matplotlib.rc('ytick', labelsize=25) \n",
    "plt.rc('font', size=25)\n",
    "\n",
    "\n",
    "\n",
    "wbk=np.loadtxt('b1.dat')[:,0]\n",
    "g1, g2, g3 = [np.loadtxt(f)[:, 2] for f in ['b1.dat', 'b2.dat', 'b3.dat']]\n",
    "l1, l2, l3 = [np.loadtxt(f)[:, 3] for f in ['b1.dat', 'b2.dat', 'b3.dat']]\n",
    "\n",
    "w_max, w_min, w_step = 0.7, 0.1, 0.001\n",
    "ww = np.round(np.arange(w_min, w_max, w_step), 3) \n",
    "\n",
    "lf1, lf2, lf3, gg1, gg2, gg3 = [np.zeros(len(ww)) for i in range(6)]\n",
    "\n",
    "for i, ww_ in enumerate(ww):\n",
    "    lf1[i], lf2[i], lf3[i] = [np.mean(l[wbk == ww_]) for l in [l1, l2, l3]]\n",
    "    gg1[i], gg2[i], gg3[i] = [np.mean(g[wbk == ww_]) for g in [g1, g2, g3]]\n",
    "\n",
    "\n",
    "length = len(ww)\n",
    "brk_indices = np.arange(0, len(wbk), length)\n",
    "wbk_reshape = wbk.reshape(-1, length)\n",
    "l_reshape = [l.reshape(-1, length) for l in [l1, l2, l3]]\n",
    "min_indices = [np.argmin(l, axis=1) for l in l_reshape]\n",
    "selected_indices = [np.random.choice(min_idx, size=len(brk_idx)) for min_idx, brk_idx in zip(min_indices, [brk_indices]*3)]\n",
    "brk = [wbk_rsh[np.arange(len(brk_idx)), sel_idx] for wbk_rsh, sel_idx, brk_idx in zip([wbk_reshape]*3, selected_indices, [brk_indices]*3)]\n",
    "brk1, brk2, brk3 = brk\n",
    "\n",
    "\n",
    "\n",
    "g_indices = np.arange(0, len(wbk), length)\n",
    "g_reshape = [g.reshape(-1,length) for g in [g1,g2,g3]]\n",
    "gamma=[g_rsh[np.arange(len(brk_idx)), sel_idx] for g_rsh, sel_idx, brk_idx in zip(g_reshape, selected_indices, [g_indices]*3)]\n",
    "\n",
    "\n",
    "\n",
    "gamma1, gamma2, gamma3 = gamma\n",
    "\n",
    "\n",
    "# Define confidence levels\n",
    "df = 3\n",
    "s = 0.5 * chi2.ppf(0.68, df)\n",
    "s2 = 0.5 * chi2.ppf(0.95, df)\n",
    "s3 = 0.5 * chi2.ppf(0.997, df)\n",
    "\n",
    "Level=[s,s2,s3]\n",
    "\n",
    "# Calculate mean and covariance matrices\n",
    "data_list = [np.column_stack((brk1, gamma1)), np.column_stack((brk2, gamma2)), \n",
    "             np.column_stack((brk3, gamma3))]\n",
    "mean_list = [np.mean(data, axis=0) for data in data_list]\n",
    "cov_list = [np.cov(data, rowvar=False) for data in data_list]\n",
    "\n",
    "point_x = 0.3\n",
    "point_y = 3.\n",
    "\n",
    "# Create colorbar\n",
    "cmap = plt.get_cmap('Set1')\n",
    "fig, axs = plt.subplots(2,2, figsize=(20, 20))\n",
    "for i, (lf, cov, mean, gg) in enumerate(zip([lf1, lf2, lf3], [cov_list[0], cov_list[1], cov_list[2]], mean_list, [gg1, gg2, gg3])):\n",
    "    # Create figure and axis\n",
    "\n",
    "\n",
    "    # Create colorbar\n",
    "    vmin = lf.min()\n",
    "    vmax = lf.max()\n",
    "    norm = mcolors.TwoSlopeNorm(vmax=vmax, vcenter=(vmax+vmin)//2, vmin=vmin)\n",
    "    sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    sm.set_array([])\n",
    "\n",
    "    for j, conf_level in enumerate(Level):\n",
    "        lambdai_, v = np.linalg.eig(cov)\n",
    "        lambdai_ = np.sqrt(lambdai_)\n",
    "        ell1 = Ellipse(xy=mean, width=lambdai_[0]*np.sqrt(Level[0])*2,\n",
    "                        height=lambdai_[1]*np.sqrt(Level[0])*2,\n",
    "                        angle=np.rad2deg(np.arccos(v[0, 0])), fill=True, alpha=1., facecolor=cmap(0))\n",
    "        ell2 = Ellipse(xy=mean, width=lambdai_[0]*np.sqrt(Level[1])*2,\n",
    "                        height=lambdai_[1]*np.sqrt(Level[1])*2,\n",
    "                        angle=np.rad2deg(np.arccos(v[0, 0])), fill=True, alpha=0.2, facecolor=cmap(1))\n",
    "        ell3 = Ellipse(xy=mean, width=lambdai_[0]*np.sqrt(Level[2])*2,\n",
    "                        height=lambdai_[1]*np.sqrt(Level[2])*2,\n",
    "                        angle=np.rad2deg(np.arccos(v[0, 0])), fill=True, alpha=0.2, facecolor=cmap(9))\n",
    "        # Add Ellipse artists to subplot\n",
    "        if i == 0:\n",
    "                \n",
    "            axs[0, 0].add_artist(ell1)\n",
    "            axs[0, 0].add_artist(ell2)\n",
    "            axs[0, 0].add_artist(ell3)\n",
    "            axs[0, 0].set_xlim(0.1, 0.7)\n",
    "            axs[0, 0].set_ylim(1.5, 4.5)\n",
    "            axs[0, 0].plot(point_x, point_y, marker='*', markersize=25, color='k')\n",
    "            axs[0, 0].set_title('0 to 200 days ')\n",
    "            axs[0, 0].set_xlabel('$\\omega_{bk}$ (rad/day)')\n",
    "            axs[0, 0].set_ylabel('$\\gamma_{2}$')\n",
    "\n",
    "          \n",
    "        elif i == 1:\n",
    "            axs[0, 1].add_artist(ell1)\n",
    "            axs[0, 1].add_artist(ell2)\n",
    "            axs[0, 1].add_artist(ell3)\n",
    "            axs[0, 1].set_xlim(0.1, 0.7)\n",
    "            axs[0, 1].set_ylim(1.5, 4.5)\n",
    "            axs[0, 1].set_xlabel('$\\omega_{bk}$ (rad/day)')\n",
    "            axs[0, 1].set_ylabel('$\\gamma_{2}$')\n",
    "\n",
    "            axs[0, 1].set_title('200 to 400 days ')\n",
    "            axs[0, 1].plot(point_x, point_y, marker='*', markersize=25, color='k')\n",
    "\n",
    "        \n",
    "        elif i == 2:\n",
    "            axs[1, 0].add_artist(ell1)\n",
    "            axs[1, 0].add_artist(ell2)\n",
    "            axs[1, 0].add_artist(ell3)\n",
    "            axs[1, 0].set_xlim(0.1, 0.7)\n",
    "            axs[1, 0].set_ylim(1.5, 4.5)\n",
    "            axs[1, 0].set_title('400 to 600 days ')\n",
    "            axs[1, 0].set_xlabel('$\\omega_{bk}$ (rad/day)')\n",
    "            axs[1, 0].set_ylabel('$\\gamma_{2}$')\n",
    "            axs[1, 0].plot(point_x, point_y, marker='*', markersize=25, color='k')\n",
    "            axs[1, 0].set_position([0.35,0.1,0.36,0.34])\n",
    "            axs[1, 1].set_visible(False)\n",
    "            \n",
    "            \n",
    "  \n",
    "        \n",
    "cbar = fig.colorbar(sm)\n",
    "cbar.ax.set_position([0.75,0.1,0.52,0.28])\n",
    "cbar.set_label('$- \\ln \\mathcal{L}(F)$')\n",
    "            \n",
    "# Show plot\n",
    "plt.show()\n",
    "fig.savefig('bpl_seg.pdf')  \n",
    "\n",
    "#_____________ BROKEN POWER LAW INTERPOLATION ______________________\n",
    "\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=25) \n",
    "matplotlib.rc('ytick', labelsize=25) \n",
    "plt.rc('font', size=25)\n",
    "\n",
    "\n",
    "wbk=np.loadtxt('bpl_full_matrix.dat')[:,0]\n",
    "g1, g2, g3, g4 = [np.loadtxt(f)[:, 2] for f in ['bpl_lin.dat', 'bpl_cub.dat', \n",
    "                                            'bpl_gas.dat','bpl_full_matrix.dat']]\n",
    "l1, l2, l3, l4 = [np.loadtxt(f)[:, 3] for f in ['bpl_lin.dat', 'bpl_cub.dat', \n",
    "                                            'bpl_gas.dat','bpl_full_matrix.dat']]\n",
    "\n",
    "w_max, w_min, w_step = 0.7, 0.1, 0.001\n",
    "ww = np.round(np.arange(w_min, w_max, w_step), 3) \n",
    "\n",
    "lf1, lf2, lf3,lf4, gg1, gg2, gg3, gg4 = [np.zeros(len(ww)) for i in range(8)]\n",
    "\n",
    "for i, ww_ in enumerate(ww):\n",
    "    lf1[i], lf2[i], lf3[i], lf4[i] = [np.mean(l[wbk == ww_]) for l in [l1, l2, l3, l4]]\n",
    "    gg1[i], gg2[i], gg3[i], gg4[i] = [np.mean(g[wbk == ww_]) for g in [g1, g2, g3, g4]]\n",
    "\n",
    "\n",
    "length = len(ww)\n",
    "brk_indices = np.arange(0, len(wbk), length)\n",
    "wbk_reshape = wbk.reshape(-1, length)\n",
    "l_reshape = [l.reshape(-1, length) for l in [l1, l2, l3, l4]]\n",
    "min_indices = [np.argmin(l, axis=1) for l in l_reshape]\n",
    "selected_indices = [np.random.choice(min_idx, size=len(brk_idx)) for min_idx, brk_idx in zip(min_indices, [brk_indices]*4)]\n",
    "brk = [wbk_rsh[np.arange(len(brk_idx)), sel_idx] for wbk_rsh, sel_idx, brk_idx in zip([wbk_reshape]*4, selected_indices, [brk_indices]*4)]\n",
    "brk1, brk2, brk3, brk4 = brk\n",
    "\n",
    "\n",
    "\n",
    "g_indices = np.arange(0, len(wbk), length)\n",
    "g_reshape = [g.reshape(-1,length) for g in [g1,g2,g3, g4]]\n",
    "gamma=[g_rsh[np.arange(len(brk_idx)), sel_idx] for g_rsh, sel_idx, brk_idx in zip(g_reshape, selected_indices, [g_indices]*4)]\n",
    "\n",
    "\n",
    "\n",
    "gamma1, gamma2, gamma3, gamma4 = gamma\n",
    "\n",
    "\n",
    "# Define confidence levels\n",
    "df = 3\n",
    "s = 0.5 * chi2.ppf(0.68, df)\n",
    "s2 = 0.5 * chi2.ppf(0.95, df)\n",
    "s3 = 0.5 * chi2.ppf(0.997, df)\n",
    "\n",
    "Level=[s,s2,s3]\n",
    "\n",
    "# Calculate mean and covariance matrices\n",
    "data_list = [np.column_stack((brk1, gamma1)), np.column_stack((brk2, gamma2)), \n",
    "              np.column_stack((brk3, gamma3)), np.column_stack((brk4, gamma4))]\n",
    "mean_list = [np.mean(data, axis=0) for data in data_list]\n",
    "cov_list = [np.cov(data, rowvar=False) for data in data_list]\n",
    "\n",
    "point_x = 0.3\n",
    "point_y = 3.\n",
    "\n",
    "# Create colorbar\n",
    "cmap = plt.get_cmap('Set1')\n",
    "fig, axs = plt.subplots(3,2, figsize=(20, 25))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "for i, (lf, cov, mean, gg) in enumerate(zip([lf1, lf2, lf3, lf4], [cov_list[0], cov_list[1], cov_list[2], cov_list[3]], mean_list, [gg1, gg2, gg3, gg4])):\n",
    "    # Create figure and axis\n",
    "\n",
    "\n",
    "    # Create colorbar\n",
    "    vmin = lf.min()\n",
    "    vmax = lf.max()\n",
    "    norm = mcolors.TwoSlopeNorm(vmax=vmax, vcenter=(vmax+vmin)//2, vmin=vmin)\n",
    "    sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    sm.set_array([])\n",
    "\n",
    "    for j, conf_level in enumerate(Level):\n",
    "        lambdai_, v = np.linalg.eig(cov)\n",
    "        lambdai_ = np.sqrt(lambdai_)\n",
    "        ell1 = Ellipse(xy=mean, width=lambdai_[0]*np.sqrt(Level[0])*2,\n",
    "                        height=lambdai_[1]*np.sqrt(Level[0])*2,\n",
    "                        angle=np.rad2deg(np.arccos(v[0, 0])), fill=True, alpha=1., facecolor=cmap(0))\n",
    "        ell2 = Ellipse(xy=mean, width=lambdai_[0]*np.sqrt(Level[1])*2,\n",
    "                        height=lambdai_[1]*np.sqrt(Level[1])*2,\n",
    "                        angle=np.rad2deg(np.arccos(v[0, 0])), fill=True, alpha=0.2, facecolor=cmap(1))\n",
    "        ell3 = Ellipse(xy=mean, width=lambdai_[0]*np.sqrt(Level[2])*2,\n",
    "                        height=lambdai_[1]*np.sqrt(Level[2])*2,\n",
    "                         angle=np.rad2deg(np.arccos(v[0, 0])), fill=True, alpha=0.2, facecolor=cmap(9))\n",
    "#        Add Ellipse artists to subplot\n",
    "        if i == 0:\n",
    "                \n",
    "            axs[0, 0].add_artist(ell1)\n",
    "            axs[0, 0].add_artist(ell2)\n",
    "            axs[0, 0].add_artist(ell3)\n",
    "            axs[0, 0].set_xlim(0.1, 0.7)\n",
    "            axs[0, 0].set_ylim(1.5, 6.5)\n",
    "            axs[0, 0].plot(point_x, point_y, marker='*', markersize=25, color='k')\n",
    "            axs[0, 0].set_title('Linear')\n",
    "            axs[0, 0].set_xlabel('$\\omega_{bk}$ (rad/day)')\n",
    "            axs[0, 0].set_ylabel('$\\gamma_{2}$')\n",
    "\n",
    "          \n",
    "        elif i == 1:\n",
    "            axs[0, 1].add_artist(ell1)\n",
    "            axs[0, 1].add_artist(ell2)\n",
    "            axs[0, 1].add_artist(ell3)\n",
    "            axs[0, 1].set_xlim(0.1, 0.7)\n",
    "            axs[0, 1].set_ylim(1.5, 6.5)\n",
    "            axs[0, 1].set_xlabel('$\\omega_{bk}$ (rad/day)')\n",
    "            axs[0, 1].set_ylabel('$\\gamma_{2}$')\n",
    "            axs[0, 1].set_title('Cubic')\n",
    "            axs[0, 1].plot(point_x, point_y, marker='*', markersize=25, color='k')\n",
    "\n",
    "        \n",
    "        elif i == 2:\n",
    "            axs[1, 0].add_artist(ell1)\n",
    "            axs[1, 0].add_artist(ell2)\n",
    "            axs[1, 0].add_artist(ell3)\n",
    "            axs[1, 0].set_xlim(0.1, 0.6)\n",
    "            axs[1, 0].set_ylim(2., 4.5)\n",
    "            axs[1, 0].set_title('Gaussian')\n",
    "            axs[1, 0].set_xlabel('$\\omega_{bk}$ (rad/day)')\n",
    "            axs[1, 0].set_ylabel('$\\gamma_{2}$')\n",
    "            axs[1, 0].plot(point_x, point_y, marker='*', markersize=25, color='k')\n",
    "            \n",
    "            \n",
    "        elif i == 3:\n",
    "            axs[1, 1].add_artist(ell1)\n",
    "            axs[1, 1].add_artist(ell2)\n",
    "            axs[1, 1].add_artist(ell3)\n",
    "            axs[1, 1].set_xlim(0.1, 0.6)\n",
    "            axs[1, 1].set_ylim(1.5, 4.)\n",
    "            axs[1, 1].set_title('Full Matrix')\n",
    "            axs[1, 1].set_xlabel('$\\omega_{bk}$ (rad/day)')\n",
    "            axs[1, 1].set_ylabel('$\\gamma_{2}$')\n",
    "            axs[1, 1].plot(point_x, point_y, marker='*', markersize=25, color='k')\n",
    "            axs[2, 0].set_visible(False)\n",
    "            axs[2, 1].set_visible(False)\n",
    "\n",
    "\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(sm,orientation='horizontal')\n",
    "cbar.ax.set_position([0.19,0.15,0.65,0.15])\n",
    "cbar.set_label('$- \\ln \\mathcal{L}(F)$')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('bpl_int.pdf')  \n",
    "\n",
    "\n",
    "\n",
    "#________________ DIFFERENT NOISE LEVEL ___________________________\n",
    "\n",
    "\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=25) \n",
    "matplotlib.rc('ytick', labelsize=25) \n",
    "plt.rc('font', size=20)\n",
    "\n",
    "\n",
    "\n",
    "wbk=np.loadtxt('bpl_dif_noise.dat')[:,0]\n",
    "g1=np.loadtxt('bpl_dif_noise.dat')[:,1]\n",
    "g2=np.loadtxt('bpl_dif_noise.dat')[:,5]\n",
    "g3=np.loadtxt('bpl_dif_noise.dat')[:,3]\n",
    "l1=np.loadtxt('bpl_dif_noise.dat')[:,2]\n",
    "l2=np.loadtxt('bpl_dif_noise.dat')[:,6]\n",
    "l3=np.loadtxt('bpl_dif_noise.dat')[:,4]\n",
    "\n",
    "\n",
    "w_max, w_min, w_step = 0.7, 0.1, 0.001\n",
    "ww = np.round(np.arange(w_min, w_max, w_step), 3) \n",
    "\n",
    "lf1, lf2, lf3, gg1, gg2, gg3 = [np.zeros(len(ww)) for i in range(6)]\n",
    "\n",
    "for i, ww_ in enumerate(ww):\n",
    "    lf1[i], lf2[i], lf3[i] = [np.mean(l[wbk == ww_]) for l in [l1, l2, l3]]\n",
    "    gg1[i], gg2[i], gg3[i] = [np.mean(g[wbk == ww_]) for g in [g1, g2, g3]]\n",
    "\n",
    "\n",
    "length = len(ww)\n",
    "brk_indices = np.arange(0, len(wbk), length)\n",
    "wbk_reshape = wbk.reshape(-1, length)\n",
    "l_reshape = [l.reshape(-1, length) for l in [l1, l2, l3]]\n",
    "min_indices = [np.argmin(l, axis=1) for l in l_reshape]\n",
    "selected_indices = [np.random.choice(min_idx, size=len(brk_idx)) for min_idx, brk_idx in zip(min_indices, [brk_indices]*3)]\n",
    "brk = [wbk_rsh[np.arange(len(brk_idx)), sel_idx] for wbk_rsh, sel_idx, brk_idx in zip([wbk_reshape]*3, selected_indices, [brk_indices]*3)]\n",
    "brk1, brk2, brk3 = brk\n",
    "\n",
    "\n",
    "\n",
    "g_indices = np.arange(0, len(wbk), length)\n",
    "g_reshape = [g.reshape(-1,length) for g in [g1,g2,g3]]\n",
    "gamma=[g_rsh[np.arange(len(brk_idx)), sel_idx] for g_rsh, sel_idx, brk_idx in zip(g_reshape, selected_indices, [g_indices]*3)]\n",
    "\n",
    "\n",
    "\n",
    "gamma1, gamma2, gamma3 = gamma\n",
    "\n",
    "\n",
    "# Define confidence levels\n",
    "df = 3\n",
    "s = 0.5 * chi2.ppf(0.68, df)\n",
    "s2 = 0.5 * chi2.ppf(0.95, df)\n",
    "s3 = 0.5 * chi2.ppf(0.997, df)\n",
    "\n",
    "Level=[s,s2,s3]\n",
    "\n",
    "# Calculate mean and covariance matrices\n",
    "data_list = [np.column_stack((brk1, gamma1)), np.column_stack((brk2, gamma2)), \n",
    "             np.column_stack((brk3, gamma3))]\n",
    "mean_list = [np.mean(data, axis=0) for data in data_list]\n",
    "cov_list = [np.cov(data, rowvar=False) for data in data_list]\n",
    "\n",
    "point_x = 0.3\n",
    "point_y = 3.\n",
    "\n",
    "# Create colorbar\n",
    "cmap = plt.get_cmap('Set1')\n",
    "fig, axs = plt.subplots(2,2, figsize=(20, 20))\n",
    "for i, (lf, cov, mean, gg) in enumerate(zip([lf1, lf2, lf3], [cov_list[0], cov_list[1], cov_list[2]], mean_list, [gg1, gg2, gg3])):\n",
    "    # Create figure and axis\n",
    "\n",
    "\n",
    "    # Create colorbar\n",
    "    vmin = lf.min()\n",
    "    vmax = lf.max()\n",
    "    norm = mcolors.TwoSlopeNorm(vmax=vmax, vcenter=(vmax+vmin)//2, vmin=vmin)\n",
    "    sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    sm.set_array([])\n",
    "\n",
    "    for j, conf_level in enumerate(Level):\n",
    "        lambdai_, v = np.linalg.eig(cov)\n",
    "        lambdai_ = np.sqrt(lambdai_)\n",
    "        ell1 = Ellipse(xy=mean, width=lambdai_[0]*np.sqrt(Level[0])*2,\n",
    "                        height=lambdai_[1]*np.sqrt(Level[0])*2,\n",
    "                        angle=np.rad2deg(np.arccos(v[0, 0])), fill=True, alpha=1., facecolor=cmap(0))\n",
    "        ell2 = Ellipse(xy=mean, width=lambdai_[0]*np.sqrt(Level[1])*2,\n",
    "                        height=lambdai_[1]*np.sqrt(Level[1])*2,\n",
    "                        angle=np.rad2deg(np.arccos(v[0, 0])), fill=True, alpha=0.2, facecolor=cmap(1))\n",
    "        ell3 = Ellipse(xy=mean, width=lambdai_[0]*np.sqrt(Level[2])*2,\n",
    "                        height=lambdai_[1]*np.sqrt(Level[2])*2,\n",
    "                        angle=np.rad2deg(np.arccos(v[0, 0])), fill=True, alpha=0.2, facecolor=cmap(9))\n",
    "        # Add Ellipse artists to subplot\n",
    "        if i == 0:\n",
    "                \n",
    "            axs[0, 0].add_artist(ell1)\n",
    "            axs[0, 0].add_artist(ell2)\n",
    "            axs[0, 0].add_artist(ell3)\n",
    "            axs[0, 0].set_xlim(0.07, 0.5)\n",
    "            axs[0, 0].set_ylim(1.5, 3.5)\n",
    "            axs[0, 0].plot(point_x, point_y, marker='*', markersize=25, color='k')\n",
    "            axs[0, 0].set_title(r'$\\frac{1}{2}$ $\\times$ True Noise Level')\n",
    "            axs[0, 0].set_xlabel('$\\omega_{bk}$ (rad/day)')\n",
    "            axs[0, 0].set_ylabel('$\\gamma_{2}$')\n",
    "\n",
    "          \n",
    "        elif i == 1:\n",
    "            axs[0, 1].add_artist(ell1)\n",
    "            axs[0, 1].add_artist(ell2)\n",
    "            axs[0, 1].add_artist(ell3)\n",
    "            axs[0, 1].set_xlim(0.1, 0.6)\n",
    "            axs[0, 1].set_ylim(1., 9.5)\n",
    "            axs[0, 1].set_xlabel('$\\omega_{bk}$ (rad/day)')\n",
    "            axs[0, 1].set_ylabel('$\\gamma_{2}$')\n",
    "\n",
    "            axs[0, 1].set_title(r'$\\frac{3}{2}$ $\\times$ True Noise Level')\n",
    "            axs[0, 1].plot(point_x, point_y, marker='*', markersize=25, color='k')\n",
    "\n",
    "        \n",
    "        elif i == 2:\n",
    "            axs[1, 0].add_artist(ell1)\n",
    "            axs[1, 0].add_artist(ell2)\n",
    "            axs[1, 0].add_artist(ell3)\n",
    "            axs[1, 0].set_xlim(0.1, 0.57)\n",
    "            axs[1, 0].set_ylim(2.0, 4.)\n",
    "            axs[1, 0].set_title('True Noise Level')\n",
    "            axs[1, 0].set_xlabel('$\\omega_{bk}$ (rad/day)')\n",
    "            axs[1, 0].set_ylabel('$\\gamma_{2}$')\n",
    "            axs[1, 0].plot(point_x, point_y, marker='*', markersize=25, color='k')\n",
    "            axs[1, 0].set_position([0.35,0.1,0.35,0.28])\n",
    "            axs[1, 1].set_visible(False)\n",
    "            \n",
    "            \n",
    "  \n",
    "        \n",
    "cbar = fig.colorbar(sm)\n",
    "cbar.ax.set_position([0.75,0.1,0.52,0.28])\n",
    "cbar.set_label('$- \\ln \\mathcal{L}(F)$')\n",
    "            \n",
    "# Show plot\n",
    "plt.show()\n",
    "fig.savefig('bpl_dif_noise.pdf')  \n",
    "\n",
    "\n",
    "#________________ LOG NOISE LEVEL ___________________________\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=18) \n",
    "matplotlib.rc('ytick', labelsize=25) \n",
    "plt.rc('font', size=40)\n",
    "\n",
    "\n",
    "data = np.loadtxt('bpl_dif_noise.dat')\n",
    "\n",
    "wbk=data[:,0]\n",
    "\n",
    "g = []\n",
    "l = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    g.append(data[:, 2 * i - 1])\n",
    "    l.append(data[:, 2 * i])\n",
    "\n",
    "g1, g2, g3, g4, g5, g6, g7, g8, g9, g10 = g\n",
    "l1, l2, l3, l4, l5, l6, l7, l8, l9, l10 = l\n",
    "\n",
    "\n",
    "w_max, w_min, w_step = 0.6, 0.1, 0.001\n",
    "ww = np.round(np.arange(w_min, w_max, w_step), 3) \n",
    "\n",
    "lf1, lf2, lf3, lf4, lf5, lf6, lf7, lf8, lf9, lf10, gg1, gg2, gg3, gg4, gg5, gg6, gg7, gg8, gg9, gg10 = [np.zeros(len(ww)) for i in range(20)]\n",
    "\n",
    "for i, ww_ in enumerate(ww):\n",
    "    lf1[i], lf2[i], lf3[i], lf4[i], lf5[i], lf6[i], lf7[i], lf8[i], lf9[i], lf10[i] = [np.mean(l[wbk == ww_]) for l in [l1, l2, l3, l4, l5, l6, l7, l8, l9, l10]]\n",
    "    gg1[i], gg2[i], gg3[i], gg4[i], gg5[i], gg6[i], gg7[i], gg8[i], gg9[i], gg10[i] = [np.mean(g[wbk == ww_]) for g in [g1, g2, g3, g4, g5, g6, g7, g8, g9, g10]]\n",
    "\n",
    "\n",
    "length = len(ww)\n",
    "brk_indices = np.arange(0, len(wbk), length)\n",
    "wbk_reshape = wbk.reshape(-1, length)\n",
    "l_reshape = [l.reshape(-1, length) for l in [l1, l2, l3, l4, l5, l6, l7, l8, l9, l10]]\n",
    "min_indices = [np.argmin(l, axis=1) for l in l_reshape]\n",
    "selected_indices = [np.random.choice(min_idx, size=len(brk_idx)) for min_idx, brk_idx in zip(min_indices, [brk_indices]*10)]\n",
    "brk = [wbk_rsh[np.arange(len(brk_idx)), sel_idx] for wbk_rsh, sel_idx, brk_idx in zip([wbk_reshape]*10, selected_indices, [brk_indices]*10)]\n",
    "brk1, brk2, brk3, brk4, brk5, brk6, brk7, brk8, brk9, brk10 = brk\n",
    "\n",
    "\n",
    "g_indices = np.arange(0, len(wbk), length)\n",
    "g_reshape = [g.reshape(-1,length) for g in [g1,g2,g3,g4,g5,g6,g7,g8,g9,g10]]\n",
    "gamma=[g_rsh[np.arange(len(brk_idx)), sel_idx] for g_rsh, sel_idx, brk_idx in zip(g_reshape, selected_indices, [g_indices]*10)]\n",
    "\n",
    "\n",
    "\n",
    "gamma1, gamma2, gamma3, gamma4, gamma5, gamma6, gamma7, gamma8, gamma9, gamma10 = gamma\n",
    "\n",
    "\n",
    "w0=0.3 # true break frequency\n",
    "slope=3.0 # true high frequency slope\n",
    "\n",
    "x=np.array([0.5, 0.57009, 0.64565, 0.72654, 0.81283, 0.90485, 1.002, 1.1049, 1.2144, 2])\n",
    "y=(1/w0)*np.mean(brk,axis=1)  # ratio of the fitted break frequency to the true break frequency\n",
    "y_err=np.std(brk,axis=1) # error bars on the fitted break frequency\n",
    "z=np.mean(gamma,axis=1)/slope # ratio of the fitted slope / true slope \n",
    "z_err=(1/slope)*np.std(gamma,axis=1) # error bars on the fitted slope\n",
    "\n",
    "# Overplotting the results from a single simulation\n",
    "\n",
    "# Convert the 'brk' and 'gamma' lists into NumPy arrays\n",
    "\n",
    "brk = np.array(brk)\n",
    "gamma = np.array(gamma)\n",
    "\n",
    "# Calculate the mean of each row\n",
    "row_means = np.mean(brk, axis=1)\n",
    "\n",
    "# Calculate the differences between each element and the mean for each row\n",
    "diffs = np.abs(brk - row_means[:, np.newaxis])\n",
    "\n",
    "# Calculate the combined differences for each index\n",
    "combined_diffs = np.sum(diffs, axis=0)\n",
    "\n",
    "# Find the index closest to the mean for each row\n",
    "closest_indices = np.abs(brk - row_means[:, np.newaxis]).argmin(axis=1)\n",
    "\n",
    "# Use the closest_indices to extract the corresponding values from each row\n",
    "single_sim_wbk = (1/w0) * brk[np.arange(brk.shape[0]), closest_indices]\n",
    "single_sim_slp = (1/slope) * gamma[np.arange(brk.shape[0]), closest_indices]\n",
    "\n",
    "\n",
    "# Create the subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30, 15))\n",
    "\n",
    "# First subplot\n",
    "ax1.plot(x, y, marker='o', color='blue', linewidth=4, label='Mean of 100 simulations')\n",
    "ax1.fill_between(x, y-y_err, y+y_err, color='red', alpha=0.4,label=r'$1-\\sigma$ deviation of 100 simulations')\n",
    "ax1.plot(x, single_sim_wbk, marker='d', color='k', linewidth=4, label='Single Simulation')\n",
    "ax1.set_xlabel(r'Relative noise level in units of $\\sigma_{F}$')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_ylabel(r'$\\frac{\\omega_{(best-fit)}}{\\omega_{true}}$')\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "ax1.set_xticks([0.5, 0.81283, 1.002,  1.2144, 2])\n",
    "ax1.set_xticklabels([0.5, 0.81283, 1.002,  1.2144, 2])\n",
    "\n",
    "ax1.legend(loc='upper left', fontsize=22)\n",
    "\n",
    "\n",
    "# Second subplot\n",
    "ax2.plot(x, z, marker='o', color='blue', linewidth=4)\n",
    "ax2.fill_between(x, z-z_err, z+z_err, color='red', alpha=0.4)  # Add shaded error region\n",
    "ax2.plot(x, single_sim_slp, marker='d', color='k', linewidth=4)\n",
    "ax2.set_xlabel(r'Relative noise level in units of $\\sigma_{F}$')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_ylabel(r'$\\frac{\\gamma_{2 (best-fit)} }{ \\gamma_{2 (true)}}$')\n",
    "ax2.grid(True)\n",
    "ax2.set_xticks([0.5, 0.81283, 1.002,  1.2144, 2])\n",
    "ax2.set_xticklabels([0.5, 0.81283, 1.002,  1.2144, 2])\n",
    "\n",
    "# Adjust the spacing between the subplots\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "# Save and display the plot\n",
    "plt.savefig(\"slope_omega_noise.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5168df",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Trebuchet MS', cursive, sans-serif; font-weight: bold; color: red;font-size: 48px;\">For The Kepler Sample</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018655e",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Verdana', cursive, sans-serif; font-weight: bold; color: green;font-size: 40px;\">For The Power Law PSD Model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90906f",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Calibri', cursive, sans-serif; font-weight: bold; color: violet;font-size: 36px;\">Time Domain Analysis</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc7c99",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Palatino', cursive, sans-serif; font-weight: bold; color: blue;font-size: 30px;\">Effect of Different Noise Level Prescriptions and Binning Timescales</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a4275",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() # get the current working directory\n",
    "\n",
    "for filename in os.listdir(cwd):\n",
    "    if filename.startswith('lcwerrors'):\n",
    "        file_id = filename.split('_')[1].split('.')[0]\n",
    "        kepler = np.loadtxt(filename)       \n",
    "       \n",
    "        t=np.round(kepler[:,0],4)\n",
    "        lc=kepler[:,1]\n",
    "\n",
    "        step=np.round(np.diff(t),4)\n",
    "        \n",
    "        bin_sizes=np.array([48])\n",
    "        \n",
    "        for bin_size in bin_sizes:\n",
    "            \n",
    "            points=len(lc)\n",
    "            tolerance=90000.05\n",
    "            cadence=np.min(step)\n",
    "\n",
    "\n",
    "            # +1 is added so that the splitting is done after the last element which \n",
    "            # satisfies the condition. if not added, the last element will not be \n",
    "            # included in the range. \n",
    "    \n",
    "            seq = np.split(t, np.where(step > tolerance*np.min(step))[0] +1)\n",
    "    \n",
    "            l = []\n",
    "    \n",
    "            for s in seq:\n",
    "                if len(s) >= points:\n",
    "                    l.append((s[0], s[-1]))\n",
    "                    \n",
    "            for x in range (len(l)):\n",
    "                    \n",
    "            # section to find the starting and the end indices of the time segments\n",
    "                    \n",
    "                def t_ini(x):\n",
    "                \n",
    "                    starting_index=int(np.where(t==np.min(l[x]))[0])\n",
    "                \n",
    "                    return starting_index\n",
    "    \n",
    "                def t_fin(x):\n",
    "                \n",
    "                    ending_index=int(np.where(t==np.max(l[x]))[0])\n",
    "                \n",
    "                    return ending_index\n",
    "    \n",
    "    \n",
    "    \n",
    "                t_i=t_ini(x)\n",
    "                t_f=t_fin(x)    \n",
    "                \n",
    "                \n",
    "                f=interpolate.interp1d(t[t_i:t_f+1],lc[t_i:t_f+1],kind='linear')\n",
    "                       \n",
    "                # # evenly spaced time series segments\n",
    "                  \n",
    "                T=np.arange(t[t_i],t[t_f],cadence)\n",
    "    \n",
    "                # new flux after interpolation\n",
    "    \n",
    "                Ft_=f(T)\n",
    "            \n",
    "               \n",
    "                length=len(Ft_)\n",
    "                   \n",
    "                index=int(points//bin_size)*bin_size # index upto which Ft1 is divisible by binning size\n",
    "                   \n",
    "                Ft1=Ft_[:index]\n",
    "           \n",
    "    \n",
    "                Ft=np.mean(Ft1.reshape(-1,bin_size),axis=1)\n",
    "            \n",
    "                N=len(Ft)\n",
    "                 \n",
    "                w=2*(np.pi)*np.linspace(1,N,N)/N/cadence\n",
    "                \n",
    "                def power (gamma):\n",
    "                    p = w**(-gamma)\n",
    "                    return p\n",
    "                \n",
    "            # Noise level prescriptions\n",
    "            \n",
    "                sigma_F=np.abs(np.mean(np.abs((np.diff(Ft))))/np.sqrt(bin_size))  # No 1\n",
    "                \n",
    "          #      sigma_F=np.abs(np.mean(np.abs((np.sqrt(Ft))))/np.sqrt(bin_size))  # No 2\n",
    "                \n",
    "          #      sigma_F = np.abs(np.mean(np.abs(kepler[:,2])))                     # No 3\n",
    "                \n",
    "          #      sigma_F = 1.1*(np.abs(np.mean(np.abs((np.diff(Ft))))/np.sqrt(bin_size))) \n",
    "    \n",
    "                \n",
    "                def Sigma_phi(alpha, gamma):\n",
    "                    Sigma_phi_ = (alpha**2) * power(gamma)\n",
    "                    return Sigma_phi_\n",
    "                \n",
    "                \n",
    "                def cov_mat(alpha, gamma):\n",
    "                    \n",
    "                    dt = np.linspace(0, N*bin_size*cadence, N)\n",
    "                    cov = Sigma_phi(alpha, gamma)\n",
    "    \n",
    "        # calculate sigma for all dt values\n",
    "                    sigma = 2 * np.trapz(cov * np.cos(w * dt[:, np.newaxis]), w, axis=1)\n",
    "    \n",
    "        # add noise to the first element of sigma\n",
    "                    sigma[0] += sigma_F**2\n",
    "    \n",
    "        # create the covariance matrix by subtracting the absolute difference between indices\n",
    "                    ID = np.arange(sigma.size)\n",
    "                    sigma = sigma[np.abs(ID - ID[:, None])]\n",
    "    \n",
    "                    return sigma\n",
    "                \n",
    "                def log_like_F (alpha,gamma):\n",
    "            \n",
    "                    mu=np.mean(Ft)\n",
    "              \n",
    "                    data=Ft-mu\n",
    "            \n",
    "                    inv=np.linalg.inv(cov_mat(alpha,gamma))\n",
    "            \n",
    "                    sign,logdet=np.linalg.slogdet(2*np.pi*cov_mat(alpha,gamma))\n",
    "            \n",
    "                    lh=-0.5*( logdet + np.transpose(data) @ inv @ data )\n",
    "            \n",
    "                    return lh\n",
    "                \n",
    "                \n",
    "                def mnz(args):\n",
    "                    alpha, gamma = args\n",
    "                    return -log_like_F(alpha, gamma) + log_like_F(1000, 1)\n",
    "    \n",
    "    \n",
    "                g1 = np.arange(1., 4.5, 0.1)\n",
    "    \n",
    "                diffe = np.zeros(len(g1))\n",
    "                amp = np.zeros(len(g1))\n",
    "    \n",
    "                results = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "                  delayed(minimize)(mnz, (1, g1_), bounds=((0.000001, 100000), (g1_, g1_)))\n",
    "                  for g1_ in g1\n",
    "                )\n",
    "    \n",
    "                for j, res1 in enumerate(results):\n",
    "                    amp[j] = res1.x[0]\n",
    "                    diffe[j] = -log_like_F(amp[j], g1[j]) + log_like_F(1000, 1)\n",
    "    \n",
    "    \n",
    "                rows=zip(g1,amp,diffe)\n",
    "                \n",
    "                \n",
    "                with open(f'diff_bin_{bin_size}_{file_id}.dat', 'a') as f:\n",
    "    #            with open(f'count_bin_{bin_size}_{file_id}.dat', 'a') as f:\n",
    "    #            with open(f'smith_bin_{bin_size}_{file_id}.dat', 'a') as f:\n",
    "                    writer = csv.writer(f, delimiter=' ')\n",
    "                    for i in rows:\n",
    "                        writer.writerow(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2138ac",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Palatino', cursive, sans-serif; font-weight: bold; color: blue;font-size: 30px;\">Effect of Incorrect Noise Level Assumptions</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1684f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() # get the current working directory\n",
    "\n",
    "for filename in os.listdir(cwd):\n",
    "    if filename.startswith('lcwerrors'):\n",
    "        file_id = filename.split('_')[1].split('.')[0]\n",
    "        kepler = np.loadtxt(filename)       \n",
    "       \n",
    "        t=np.round(kepler[:,0],4)\n",
    "        lc=kepler[:,1]\n",
    "\n",
    "        step=np.round(np.diff(t),4)\n",
    "        \n",
    "        bin_sizes=np.array([1])\n",
    "        \n",
    "        for bin_size in bin_sizes:\n",
    "            \n",
    "            \n",
    "            points=1000\n",
    "            tolerance=3.05 \n",
    "            cadence=np.min(step)\n",
    "\n",
    "\n",
    "            # +1 is added so that the splitting is done after the last element which \n",
    "            # satisfies the condition. if not added, the last element will not be \n",
    "            # included in the range. \n",
    "    \n",
    "            seq = np.split(t, np.where(step > tolerance*np.min(step))[0] +1)\n",
    "    \n",
    "            l = []\n",
    "    \n",
    "            for s in seq:\n",
    "                if len(s) > points:\n",
    "                    l.append((np.min(s), np.max(s)))\n",
    "                    \n",
    "            for x in range (len(l)):\n",
    "                    \n",
    "            # section to find the starting and the end indices of the time segments\n",
    "                    \n",
    "                def t_ini(x):\n",
    "                \n",
    "                    starting_index=int(np.where(t==np.min(l[x]))[0])\n",
    "                \n",
    "                    return starting_index\n",
    "    \n",
    "                def t_fin(x):\n",
    "                \n",
    "                    ending_index=int(np.where(t==np.max(l[x]))[0])\n",
    "                \n",
    "                    return ending_index\n",
    "    \n",
    "    \n",
    "    \n",
    "                t_i=t_ini(x)\n",
    "                t_f=t_fin(x)    \n",
    "                \n",
    "                \n",
    "                f=interpolate.interp1d(t[t_i:t_f+1],lc[t_i:t_f+1],kind='linear')\n",
    "                       \n",
    "                # # evenly spaced time series segments\n",
    "                  \n",
    "                T=np.arange(t[t_i],t[t_f],cadence)\n",
    "    \n",
    "                # new flux after interpolation\n",
    "    \n",
    "                Ft_=f(T)\n",
    "            \n",
    "               \n",
    "                length=len(Ft_)\n",
    "                   \n",
    "                index=bin_size*(length//bin_size) # index upto which Ft1 is divisible by binning size\n",
    "                   \n",
    "                Ft1=Ft_[:index]\n",
    "           \n",
    "    \n",
    "                Ft=np.mean(Ft1.reshape(-1,bin_size),axis=1)\n",
    "            \n",
    "                N=len(Ft)\n",
    "                 \n",
    "                w=2*(np.pi)*np.linspace(1,N,N)/N/cadence\n",
    "                \n",
    "                def power (gamma):\n",
    "                    p = w**(-gamma)\n",
    "                    return p\n",
    "                \n",
    "            # Noise level prescriptions\n",
    "            \n",
    "          #      sigma_F=np.abs(np.mean(np.abs((np.diff(Ft))))/np.sqrt(bin_size))  # No 1\n",
    "                \n",
    "          #      sigma_F=np.abs(np.mean(np.abs((np.sqrt(Ft))))/np.sqrt(bin_size))  # No 2\n",
    "                \n",
    "                sigma_F = np.abs(np.mean(np.abs(kepler[:,2])))                     # No 3\n",
    "    \n",
    "                \n",
    "                def Sigma_phi(alpha, gamma):\n",
    "                    Sigma_phi_ = (alpha**2) * power(gamma)\n",
    "                    return Sigma_phi_\n",
    "                \n",
    "                \n",
    "                def cov_mat(alpha, gamma, sigma_F):\n",
    "                    \n",
    "                    dt = np.linspace(0, N*bin_size*cadence, N)\n",
    "                    cov = Sigma_phi(alpha, gamma)\n",
    "    \n",
    "        # calculate sigma for all dt values\n",
    "                    sigma = 2 * np.trapz(cov * np.cos(w * dt[:, np.newaxis]), w, axis=1)\n",
    "    \n",
    "        # add noise to the first element of sigma\n",
    "                    sigma[0] += sigma_F**2\n",
    "    \n",
    "        # create the covariance matrix by subtracting the absolute difference between indices\n",
    "                    ID = np.arange(sigma.size)\n",
    "                    sigma = sigma[np.abs(ID - ID[:, None])]\n",
    "    \n",
    "                    return sigma\n",
    "                \n",
    "                def log_like_F (alpha,gamma,sigma_F):\n",
    "            \n",
    "                    mu=np.mean(Ft)\n",
    "              \n",
    "                    data=Ft-mu\n",
    "            \n",
    "                    inv=np.linalg.inv(cov_mat(alpha,gamma,sigma_F))\n",
    "            \n",
    "                    sign,logdet=np.linalg.slogdet(2*np.pi*cov_mat(alpha,gamma,sigma_F))\n",
    "            \n",
    "                    lh=-0.5*( logdet + np.transpose(data) @ inv @ data )\n",
    "            \n",
    "                    return lh\n",
    "                \n",
    "                \n",
    "                def mnz(args):\n",
    "                    alpha, gamma, sigma_F = args\n",
    "                    return -log_like_F(alpha, gamma, sigma_F) + log_like_F(10, 1, sigma_F)\n",
    "    \n",
    "    \n",
    "                g1 = np.arange(1., 4.5, 0.1)\n",
    "    \n",
    "                # Noise level from 0.5 to 2.0 in 10 log steps\n",
    "                \n",
    "                ss1, ss2, ss3, ss4, ss5, ss6, ss7, ss8, ss9, ss10 = np.array([0.5, 0.57009, 0.64565, 0.72654, 0.81283, 0.90485, 1.002, 1.1049, 1.2144, 2]) * sigma_F\n",
    "                diffe1, diffe2, diffe3, diffe4, diffe5, diffe6, diffe7, diffe8, diffe9, diffe10  = [np.zeros(len(g1)) for i in range(10)]\n",
    "                amp1, amp2, amp3, amp4, amp5, amp6, amp7, amp8, amp9, amp10 = [np.zeros(len(g1)) for i in range(10)]\n",
    "              \n",
    "                \n",
    "              \n",
    "                results1 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "                    delayed(minimize)(mnz, (1, g1_,ss1), bounds=((0.1, 10), (g1_, g1_),(ss1,ss1)))\n",
    "                    for g1_ in g1\n",
    "                )\n",
    "\n",
    "                for j, res1 in enumerate(results1):\n",
    "                    amp1[j] = res1.x[0]\n",
    "                    diffe1[j] = -log_like_F(amp1[j], g1[j],ss1) + log_like_F(1, 1,ss1)\n",
    "                    \n",
    "                    \n",
    "                results2 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "                    delayed(minimize)(mnz, (1, g1_,ss2), bounds=((0.1, 10), (g1_, g1_),(ss2,ss2)))\n",
    "                    for g1_ in g1\n",
    "                )\n",
    "\n",
    "                for j, res2 in enumerate(results2):\n",
    "                    amp2[j] = res2.x[0]\n",
    "                    diffe2[j] = -log_like_F(amp2[j], g1[j],ss2) + log_like_F(1, 1,ss2)\n",
    "                    \n",
    "              \n",
    "                results3 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "                    delayed(minimize)(mnz, (1, g1_,ss3), bounds=((0.1, 10), (g1_, g1_),(ss3,ss3)))\n",
    "                    for g1_ in g1\n",
    "                )\n",
    "\n",
    "                for j, res3 in enumerate(results3):\n",
    "                    amp3[j] = res3.x[0]\n",
    "                    diffe3[j] = -log_like_F(amp3[j], g1[j],ss3) + log_like_F(1, 1,ss3)\n",
    "                    \n",
    "                    \n",
    "                results4 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "                    delayed(minimize)(mnz, (1, g1_,ss4), bounds=((0.1, 10), (g1_, g1_),(ss4,ss4)))\n",
    "                    for g1_ in g1\n",
    "                )\n",
    "\n",
    "                for j, res4 in enumerate(results4):\n",
    "                    amp4[j] = res4.x[0]\n",
    "                    diffe4[j] = -log_like_F(amp4[j], g1[j],ss4) + log_like_F(1, 1,ss4)\n",
    "                    \n",
    "                    \n",
    "                results5 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "                    delayed(minimize)(mnz, (1, g1_,ss5), bounds=((0.1, 10), (g1_, g1_),(ss5,ss5)))\n",
    "                    for g1_ in g1\n",
    "                )\n",
    "\n",
    "                for j, res5 in enumerate(results5):\n",
    "                    amp5[j] = res5.x[0]\n",
    "                    diffe5[j] = -log_like_F(amp5[j], g1[j],ss5) + log_like_F(1, 1,ss5)\n",
    "                    \n",
    "                    \n",
    "                results6 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "                    delayed(minimize)(mnz, (1, g1_,ss6), bounds=((0.1, 10), (g1_, g1_),(ss6,ss6)))\n",
    "                    for g1_ in g1\n",
    "                )\n",
    "\n",
    "                for j, res6 in enumerate(results6):\n",
    "                    amp6[j] = res6.x[0]\n",
    "                    diffe6[j] = -log_like_F(amp6[j], g1[j],ss6) + log_like_F(1, 1,ss6)\n",
    "                    \n",
    "              \n",
    "                results7 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "                    delayed(minimize)(mnz, (1, g1_,ss7), bounds=((0.1, 10), (g1_, g1_),(ss7,ss7)))\n",
    "                    for g1_ in g1\n",
    "                )\n",
    "\n",
    "                for j, res7 in enumerate(results7):\n",
    "                    amp7[j] = res7.x[0]\n",
    "                    diffe7[j] = -log_like_F(amp7[j], g1[j],ss7) + log_like_F(1, 1,ss7)\n",
    "                    \n",
    "                    \n",
    "                results8 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "                    delayed(minimize)(mnz, (1, g1_,ss8), bounds=((0.1, 10), (g1_, g1_),(ss8,ss8)))\n",
    "                    for g1_ in g1\n",
    "                )\n",
    "\n",
    "                for j, res8 in enumerate(results8):\n",
    "                    amp8[j] = res8.x[0]\n",
    "                    diffe8[j] = -log_like_F(amp8[j], g1[j],ss8) + log_like_F(1, 1,ss8)\n",
    "                    \n",
    "                    \n",
    "                results9 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "                    delayed(minimize)(mnz, (1, g1_,ss8), bounds=((0.1, 10), (g1_, g1_),(ss9,ss9)))\n",
    "                    for g1_ in g1\n",
    "                )\n",
    "\n",
    "                for j, res9 in enumerate(results9):\n",
    "                    amp9[j] = res9.x[0]\n",
    "                    diffe9[j] = -log_like_F(amp9[j], g1[j],ss9) + log_like_F(1, 1,ss9)\n",
    "                    \n",
    "                    \n",
    "                results10 = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "                    delayed(minimize)(mnz, (1, g1_,ss10), bounds=((0.1, 10), (g1_, g1_),(ss10,ss10)))\n",
    "                    for g1_ in g1\n",
    "                )\n",
    "\n",
    "                for j, res10 in enumerate(results10):\n",
    "                    amp10[j] = res10.x[0]\n",
    "                    diffe10[j] = -log_like_F(amp10[j], g1[j],ss10) + log_like_F(1, 1,ss10)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                rows=zip(np.round(g1,2),np.round(diffe1,2),np.round(diffe2,2),np.round(diffe3,2),np.round(diffe4,2),\n",
    "                         np.round(diffe5,2),np.round(diffe6,2),np.round(diffe7,2),np.round(diffe8,2),\n",
    "                         np.round(diffe9,2),np.round(diffe10,2))\n",
    "                \n",
    "                \n",
    "                with open(f'log_noise3_{bin_size}_{file_id}.dat', 'a') as f:\n",
    "                    writer = csv.writer(f, delimiter=' ')\n",
    "                    for i in rows:\n",
    "                        writer.writerow(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518fcdf",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Century Gothic', cursive, sans-serif; font-weight: bold; color: purple;font-size: 30px;\">Template for Plots</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____________ For Different Noise Level _______________________ \n",
    "\n",
    "#__________________ For Noise Level Prescription 1 __________________________\n",
    "\n",
    "file_id_list = []  # to store the file_id\n",
    "slp_dict = {}\n",
    "std_dict = {}\n",
    "bin_sizes = [1, 4, 16, 64]\n",
    "\n",
    "cwd = os.getcwd() # get the current working directory\n",
    "\n",
    "for size in bin_sizes:\n",
    "    slp_dict[size] = []\n",
    "    std_dict[size] = []\n",
    "    prefix = f\"diff_bin_{size}_\"\n",
    "    \n",
    "    for filename in os.listdir(cwd):\n",
    "        if filename.startswith(prefix):\n",
    "            file_id = filename.split('_')[3].split('.')[0]\n",
    "            file_id_list.append(file_id)\n",
    "            kepler = np.loadtxt(filename)       \n",
    "            slope=kepler[:,0]\n",
    "            ll=kepler[:,2]\n",
    "            slp=np.arange(1., 4.5, 0.1)\n",
    "            llf = np.zeros(len(slope))\n",
    "            length=len(slp)\n",
    "            for i in range(0, len(slope), length):\n",
    "                min_indices = np.where(ll[i:i+length] == np.min(ll[i:i+length]))[0]\n",
    "                random_index = np.random.choice(min_indices)\n",
    "                llf[i] = slope[i:i+length][random_index]\n",
    "                \n",
    "            llf = llf[llf != 0]\n",
    "            slp_dict[size].append(round(np.mean(llf), 2))\n",
    "            std_dict[size].append(round(np.std(llf), 2))\n",
    "\n",
    "\n",
    "file_id_list=np.unique(file_id_list)\n",
    "\n",
    "\n",
    "#__________________ For Noise Level Prescription 2 __________________________\n",
    "\n",
    "\n",
    "\n",
    "file_id_list = []  # to store the file_id\n",
    "slp_dict2 = {}\n",
    "std_dict2 = {}\n",
    "bin_sizes = [1, 4, 16, 64]\n",
    "\n",
    "cwd = os.getcwd() # get the current working directory\n",
    "\n",
    "for size in bin_sizes:\n",
    "    slp_dict2[size] = []\n",
    "    std_dict2[size] = []\n",
    "    prefix = f\"count_bin_{size}_\"\n",
    "    \n",
    "    for filename in os.listdir(cwd):\n",
    "        if filename.startswith(prefix):\n",
    "            file_id = filename.split('_')[3].split('.')[0]\n",
    "            file_id_list.append(file_id)\n",
    "            kepler = np.loadtxt(filename)       \n",
    "            slope=kepler[:,0]\n",
    "            ll=kepler[:,2]\n",
    "            slp=np.arange(1., 4.5, 0.1)\n",
    "            llf = np.zeros(len(slope))\n",
    "            length=len(slp)\n",
    "            for i in range(0, len(slope), length):\n",
    "                min_indices = np.where(ll[i:i+length] == np.min(ll[i:i+length]))[0]\n",
    "                random_index = np.random.choice(min_indices)\n",
    "                llf[i] = slope[i:i+length][random_index]\n",
    "    \n",
    "            llf = llf[llf != 0]\n",
    "            slp_dict2[size].append(round(np.mean(llf), 2))\n",
    "            std_dict2[size].append(round(np.std(llf), 2))\n",
    "\n",
    "\n",
    "file_id_list=np.unique(file_id_list)\n",
    "\n",
    "\n",
    "#__________________ For Noise Level Prescription 3 __________________________\n",
    "\n",
    "\n",
    "\n",
    "file_id_list = []  # to store the file_id\n",
    "slp_dict3 = {}\n",
    "std_dict3 = {}\n",
    "bin_sizes = [1, 4, 16, 64]\n",
    "\n",
    "cwd = os.getcwd() # get the current working directory\n",
    "\n",
    "for size in bin_sizes:\n",
    "    slp_dict3[size] = []\n",
    "    std_dict3[size] = []\n",
    "    prefix = f\"smith_bin_{size}_\"\n",
    "    \n",
    "    for filename in os.listdir(cwd):\n",
    "        if filename.startswith(prefix):\n",
    "            file_id = filename.split('_')[3].split('.')[0]\n",
    "            file_id_list.append(file_id)\n",
    "            \n",
    "            kepler = np.loadtxt(filename)       \n",
    "            slope=kepler[:,0]\n",
    "            ll=kepler[:,2]\n",
    "            slp=np.arange(1., 4.5, 0.1)\n",
    "            llf = np.zeros(len(slope))\n",
    "            length=len(slp)\n",
    "            for i in range(0, len(slope), length):\n",
    "                min_indices = np.where(ll[i:i+length] == np.min(ll[i:i+length]))[0]\n",
    "                random_index = np.random.choice(min_indices)\n",
    "                llf[i] = slope[i:i+length][random_index]\n",
    "    \n",
    "            llf = llf[llf != 0]\n",
    "            slp_dict3[size].append(round(np.mean(llf), 2))\n",
    "            std_dict3[size].append(round(np.std(llf), 2))\n",
    "\n",
    "\n",
    "file_id_list=np.unique(file_id_list)\n",
    "\n",
    "\n",
    "#______________________________Plot___________________________________________\n",
    "\n",
    "# Set the font size for tick labels\n",
    "plt.rc('xtick', labelsize=30)\n",
    "plt.rc('ytick', labelsize=30)\n",
    "plt.rc('font', size=30)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(35, 12))  # Increase the figsize to make the plots bigger\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "markers = ['o', 's', '^', '*', 'v', 'p', 'D', 'X', 'd', '<', '>', '1', '2', '3', '4', '8', 'h', 'H', '+', 'x', '|']\n",
    "bin_time = np.array(bin_sizes) * 0.5\n",
    "\n",
    "legend_labels = []  # List to store legend labels\n",
    "\n",
    "for i in range(len(slp_dict[1])):\n",
    "    axes[0].plot(bin_time, [slp_dict[1][i], slp_dict[4][i], slp_dict[16][i], slp_dict[64][i]], marker=markers[i], label=file_id_list[i])\n",
    "    axes[1].plot(bin_time, [slp_dict2[1][i], slp_dict2[4][i], slp_dict2[16][i], slp_dict2[64][i]], marker=markers[i], label=file_id_list[i])\n",
    "    axes[2].plot(bin_time, [slp_dict3[1][i], slp_dict3[4][i], slp_dict3[16][i], slp_dict3[64][i]], marker=markers[i], label=file_id_list[i])\n",
    "\n",
    "    legend_labels.append(file_id_list[i])  # Add legend label to the list\n",
    "\n",
    "axes[0].set_xlabel('Bin timescale (Hours)')\n",
    "axes[0].set_ylabel('PSD Slope')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_ylim(1.0, 4.5)\n",
    "axes[0].set_xticks(bin_time)\n",
    "axes[0].set_xticklabels([bin_time[0], bin_time[1], bin_time[2], bin_time[3]])\n",
    "\n",
    "axes[1].set_xlabel('Bin timescale (Hours)')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_ylim(1.0, 4.5)\n",
    "axes[1].set_yticks([])\n",
    "axes[1].set_xticks(bin_time)\n",
    "axes[1].set_xticklabels([bin_time[0], bin_time[1], bin_time[2], bin_time[3]])\n",
    "\n",
    "axes[2].set_xlabel('Bin timescale (Hours)')\n",
    "axes[2].set_xscale('log')\n",
    "axes[2].set_ylim(1.0, 4.5)\n",
    "axes[2].set_yticks([])\n",
    "axes[2].set_xticks(bin_time)\n",
    "axes[2].set_xticklabels([bin_time[0], bin_time[1], bin_time[2], bin_time[3]])\n",
    "\n",
    "legend = axes[2].legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=16)\n",
    "legend.set_title(\"Kepler ID\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('slp_vs_bin.pdf')\n",
    "plt.show()\n",
    "\n",
    "#__________________ For Different Noise Level Relative to the True Level ____________\n",
    "\n",
    "\n",
    "\n",
    "cwd = os.getcwd() # get the current working directory\n",
    "\n",
    "for filename in os.listdir(cwd):\n",
    "    if filename.startswith('lcwerrors'):\n",
    "        file_id = filename.split('_')[1].split('.')[0]\n",
    "        kepler = np.loadtxt(filename)       \n",
    "       \n",
    "        t=np.round(kepler[:,0],4)\n",
    "        lc=kepler[:,1]\n",
    "\n",
    "        step=np.round(np.diff(t),4)\n",
    "        \n",
    "        bin_sizes=np.array([1])\n",
    "        \n",
    "        for bin_size in bin_sizes:\n",
    "            \n",
    "            \n",
    "            points=1000\n",
    "            tolerance=3.05 \n",
    "            cadence=np.min(step)\n",
    "\n",
    "\n",
    "            # +1 is added so that the splitting is done after the last element which \n",
    "            # satisfies the condition. if not added, the last element will not be \n",
    "            # included in the range. \n",
    "    \n",
    "            seq = np.split(t, np.where(step > tolerance*np.min(step))[0] +1)\n",
    "    \n",
    "            l = []\n",
    "    \n",
    "            for s in seq:\n",
    "                if len(s) > points:\n",
    "                    l.append((np.min(s), np.max(s)))\n",
    "                    \n",
    "            for x in range (len(l)):\n",
    "                    \n",
    "            # section to find the starting and the end indices of the time segments\n",
    "                    \n",
    "                def t_ini(x):\n",
    "                \n",
    "                    starting_index=int(np.where(t==np.min(l[x]))[0])\n",
    "                \n",
    "                    return starting_index\n",
    "    \n",
    "                def t_fin(x):\n",
    "                \n",
    "                    ending_index=int(np.where(t==np.max(l[x]))[0])\n",
    "                \n",
    "                    return ending_index\n",
    "    \n",
    "    \n",
    "    \n",
    "                t_i=t_ini(x)\n",
    "                t_f=t_fin(x)    \n",
    "                \n",
    "                \n",
    "                f=interpolate.interp1d(t[t_i:t_f+1],lc[t_i:t_f+1],kind='linear')\n",
    "                       \n",
    "                # # evenly spaced time series segments\n",
    "                  \n",
    "                T=np.arange(t[t_i],t[t_f],cadence)\n",
    "    \n",
    "                # new flux after interpolation\n",
    "    \n",
    "                Ft_=f(T)\n",
    "            \n",
    "               \n",
    "                length=len(Ft_)\n",
    "                   \n",
    "                index=bin_size*(length//bin_size) # index upto which Ft1 is divisible by binning size\n",
    "                   \n",
    "                Ft1=Ft_[:index]\n",
    "           \n",
    "    \n",
    "                Ft=np.mean(Ft1.reshape(-1,bin_size),axis=1)\n",
    "        \n",
    "        sigma_F=np.abs(np.mean(np.abs((np.diff(Ft)))))\n",
    "\n",
    "\n",
    "        \n",
    "#________________ For Noise Level 1 __________________________________\n",
    "\n",
    "       \n",
    "        \n",
    "file_id_list = []  # to store the file_id\n",
    "slp_dict1 = {}\n",
    "noise_level = np.array([0.5, 0.57009, 0.64565, 0.72654, 0.81283, 0.90485, 1.002, 1.1049, 1.2144, 2]) * sigma_F\n",
    "\n",
    "prefix = f\"log_noise1_\"\n",
    "\n",
    "for filename in os.listdir(cwd):\n",
    "    if filename.startswith(prefix):\n",
    "        file_id = filename.split('_')[3].split('.')[0]\n",
    "        file_id_list.append(file_id)\n",
    "        \n",
    "        kepler = np.loadtxt(filename)\n",
    "        \n",
    "        slope = kepler[:, 0]\n",
    "        \n",
    "        for col in range(1, 11):\n",
    "            \n",
    "            ll = kepler[:, col]\n",
    "            slp = np.arange(1., 4.5, 0.1)\n",
    "            llf = np.zeros(len(slope))\n",
    "            length = len(slp)\n",
    "            for i in range(0, len(slope), length):\n",
    "                min_indices = np.where(ll[i:i+length] == np.min(ll[i:i+length]))[0]\n",
    "                random_index = np.random.choice(min_indices)\n",
    "                llf[i] = slope[i:i+length][random_index]\n",
    "\n",
    "            llf = llf[llf != 0]\n",
    "            \n",
    "            noise = noise_level[col - 1]  # Get the noise level based on column index\n",
    "            \n",
    "            if noise not in slp_dict1:\n",
    "                slp_dict1[noise] = []\n",
    "                \n",
    "            \n",
    "            slp_dict1[noise].append(round(np.mean(llf), 2))      \n",
    "        \n",
    "file_id_list=np.unique(file_id_list)        \n",
    "  \n",
    "\n",
    "\n",
    "# _________________ For Noise level 2 __________________________________      \n",
    "        \n",
    "slp_dict2 = {}\n",
    "\n",
    "prefix = f\"log_noise2_\"\n",
    "\n",
    "for filename in os.listdir(cwd):\n",
    "    if filename.startswith(prefix):\n",
    "        \n",
    "        kepler = np.loadtxt(filename)\n",
    "        \n",
    "        slope = kepler[:, 0]\n",
    "        \n",
    "        for col in range(1, 11):\n",
    "            \n",
    "            ll = kepler[:, col]\n",
    "            slp = np.arange(1., 4.5, 0.1)\n",
    "            llf = np.zeros(len(slope))\n",
    "            length = len(slp)\n",
    "            for i in range(0, len(slope), length):\n",
    "                min_indices = np.where(ll[i:i+length] == np.min(ll[i:i+length]))[0]\n",
    "                random_index = np.random.choice(min_indices)\n",
    "                llf[i] = slope[i:i+length][random_index]\n",
    "\n",
    "            llf = llf[llf != 0]\n",
    "            \n",
    "            noise = noise_level[col - 1]  # Get the noise level based on column index\n",
    "            \n",
    "            if noise not in slp_dict2:\n",
    "                slp_dict2[noise] = []\n",
    "                \n",
    "            \n",
    "            slp_dict2[noise].append(round(np.mean(llf), 2))      \n",
    "        \n",
    "        \n",
    "# _________________ For Noise level 3 __________________________________      \n",
    "        \n",
    "slp_dict3 = {}\n",
    "\n",
    "prefix = f\"log_noise3_\"\n",
    "\n",
    "for filename in os.listdir(cwd):\n",
    "    if filename.startswith(prefix):\n",
    "        \n",
    "        kepler = np.loadtxt(filename)\n",
    "        \n",
    "        slope = kepler[:, 0]\n",
    "        \n",
    "        for col in range(1, 11):\n",
    "            \n",
    "            ll = kepler[:, col]\n",
    "            slp = np.arange(1., 4.5, 0.1)\n",
    "            llf = np.zeros(len(slope))\n",
    "            length = len(slp)\n",
    "            for i in range(0, len(slope), length):\n",
    "                min_indices = np.where(ll[i:i+length] == np.min(ll[i:i+length]))[0]\n",
    "                random_index = np.random.choice(min_indices)\n",
    "                llf[i] = slope[i:i+length][random_index]\n",
    "\n",
    "            llf = llf[llf != 0]\n",
    "            \n",
    "            noise = noise_level[col - 1]  # Get the noise level based on column index\n",
    "            \n",
    "            if noise not in slp_dict3:\n",
    "                slp_dict3[noise] = []\n",
    "                \n",
    "            \n",
    "            slp_dict3[noise].append(round(np.mean(llf), 2))       \n",
    "        \n",
    "        \n",
    "#________________ Plot ______________________________________\n",
    "\n",
    "\n",
    "# Set the font size for tick labels\n",
    "plt.rc('xtick', labelsize=30)\n",
    "plt.rc('ytick', labelsize=30)\n",
    "plt.rc('font', size=30)\n",
    "\n",
    "# Extract the noise levels and values from slp_dict1\n",
    "noises = list(slp_dict1.keys())\n",
    "values1 = list(slp_dict1.values())\n",
    "values2 = list(slp_dict2.values())\n",
    "values3 = list(slp_dict3.values())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(35, 12))  # Increase the figsize to make the plots bigger\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "markers = ['o', 's', '^', '*', 'v', 'p', 'D', 'X', 'd', '<', '>', '1', '2', '3', '4', '8', 'h', 'H', '+', 'x', '|']\n",
    "\n",
    "legend_labels = []  # List to store legend labels\n",
    "\n",
    "# Iterate over the columns and plot each column with the noise level\n",
    "for i in range(len(values1[0])):\n",
    "    column1 = [row[i] for row in values1]\n",
    "    column2 = [row[i] for row in values2]\n",
    "    column3 = [row[i] for row in values3]\n",
    "    \n",
    "    # Plot the column values with the noise level, assigning markers and labels\n",
    "    axes[0].plot(noises/sigma_F, column1, marker=markers[i], label=file_id_list[i])\n",
    "    axes[1].plot(noises/sigma_F, column2, marker=markers[i], label=file_id_list[i])\n",
    "    axes[2].plot(noises/sigma_F, column3, marker=markers[i], label=file_id_list[i])\n",
    "    \n",
    "    # Add legend label to the list\n",
    "    legend_labels.append(file_id_list[i])\n",
    "\n",
    "# Set plot labels and title for the first subplot\n",
    "axes[0].set_xlabel(r'Relative noise level in units of $\\sigma_{F}$')\n",
    "axes[1].set_xlabel(r'Relative noise level in units of $\\sigma_{F}$')\n",
    "axes[2].set_xlabel(r'Relative noise level in units of $\\sigma_{F}$')\n",
    "\n",
    "axes[0].set_ylabel('PSD Slope')\n",
    "\n",
    "axes[0].set_ylim(1.0, 4.5)\n",
    "axes[1].set_ylim(1.0, 4.5)\n",
    "axes[2].set_ylim(1.0, 4.5)\n",
    "\n",
    "axes[1].set_yticks([])\n",
    "axes[2].set_yticks([])\n",
    "\n",
    "legend = axes[2].legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=16)\n",
    "legend.set_title(\"Kepler ID\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('slp_vs_noise.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc01bb",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Calibri', cursive, sans-serif; font-weight: bold; color: violet;font-size: 36px;\">Frequency Domain Analysis</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac63fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() # get the current working directory\n",
    "\n",
    "for filename in os.listdir(cwd):\n",
    "    if filename.startswith('lcwerrors'):\n",
    "        file_id = filename.split('_')[1].split('.')[0]\n",
    "        kepler = np.loadtxt(filename)       \n",
    "       \n",
    "        t=np.round(kepler[:,0],4)\n",
    "        lc=kepler[:,1]\n",
    "\n",
    "        step=np.round(np.diff(t),4)\n",
    "        \n",
    "        bin_sizes=np.array([144])\n",
    "        \n",
    "        for bin_size in bin_sizes:\n",
    "            \n",
    "            \n",
    "            points=len(lc)\n",
    "            tolerance=90000.05 \n",
    "            cadence=np.min(step)\n",
    "\n",
    "\n",
    "            # +1 is added so that the splitting is done after the last element which \n",
    "            # satisfies the condition. if not added, the last element will not be \n",
    "            # included in the range. \n",
    "    \n",
    "            seq = np.split(t, np.where(step > tolerance*np.min(step))[0] +1)\n",
    "    \n",
    "            l = []\n",
    "    \n",
    "            for s in seq:\n",
    "                if len(s) >= points:\n",
    "                    l.append((s[0], s[-1]))\n",
    "                    \n",
    "            for x in range (len(l)):\n",
    "                    \n",
    "            # section to find the starting and the end indices of the time segments\n",
    "                    \n",
    "                def t_ini(x):\n",
    "                \n",
    "                    starting_index=int(np.where(t==np.min(l[x]))[0])\n",
    "                \n",
    "                    return starting_index\n",
    "    \n",
    "                def t_fin(x):\n",
    "                \n",
    "                    ending_index=int(np.where(t==np.max(l[x]))[0])\n",
    "                \n",
    "                    return ending_index\n",
    "    \n",
    "    \n",
    "    \n",
    "                t_i=t_ini(x)\n",
    "                t_f=t_fin(x)    \n",
    "                \n",
    "                \n",
    "                f=interpolate.interp1d(t[t_i:t_f+1],lc[t_i:t_f+1],kind='linear')\n",
    "                       \n",
    "                # # evenly spaced time series segments\n",
    "                  \n",
    "                T=np.arange(t[t_i],t[t_f],cadence)\n",
    "    \n",
    "                # new flux after interpolation\n",
    "    \n",
    "                Ft_=f(T)\n",
    "            \n",
    "               \n",
    "                length=len(Ft_)\n",
    "                   \n",
    "                index=int(points//bin_size)*bin_size # index upto which Ft1 is divisible by binning size\n",
    "                   \n",
    "                Ft1=Ft_[:index]\n",
    "           \n",
    "    \n",
    "                Ft=np.mean(Ft1.reshape(-1,bin_size),axis=1)\n",
    "                \n",
    "                # Fourier transform of Ft\n",
    "                \n",
    "                F_w = np.fft.fft(Ft, norm='ortho')\n",
    "            \n",
    "                N=len(Ft)\n",
    "                 \n",
    "                w=2*(np.pi)*np.linspace(1,N,N)/N/cadence\n",
    "                \n",
    "                def power (gamma):\n",
    "                    p = w**(-gamma)\n",
    "                    return p\n",
    "                \n",
    "            # Noise level prescriptions\n",
    "            \n",
    "                sigma_F=np.abs(np.mean(np.abs((np.diff(Ft))))/np.sqrt(bin_size))  # No 1\n",
    "                \n",
    "                def Sigma_phi(alpha, gamma):\n",
    "                    Sigma_phi_ = (alpha**2) * power(gamma)\n",
    "                    return Sigma_phi_\n",
    "                \n",
    "                \n",
    "                def log_like_F (alpha,gamma):\n",
    "            \n",
    "                    Sigma_phi_ = Sigma_phi(alpha, gamma)\n",
    "                    \n",
    "                    power_F_ = (np.abs(F_w)**2)\n",
    "                    \n",
    "                    lh = -0.5*(np.sum(power_F_ / Sigma_phi_) + np.sum(np.log(2*np.pi*Sigma_phi_)))\n",
    "                    \n",
    "                    return lh\n",
    "                \n",
    "                \n",
    "                def mnz(args):\n",
    "                    alpha, gamma = args\n",
    "                    return -log_like_F(alpha, gamma) + log_like_F(2000, 1)\n",
    "    \n",
    "    \n",
    "                g1 = np.arange(1., 4.5, 0.1)\n",
    "    \n",
    "                diffe = np.zeros(len(g1))\n",
    "                amp = np.zeros(len(g1))\n",
    "    \n",
    "                results = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "                  delayed(minimize)(mnz, (1000, g1_), bounds=((100, 1000000), (g1_, g1_)))\n",
    "                  for g1_ in g1\n",
    "                )\n",
    "    \n",
    "                for j, res1 in enumerate(results):\n",
    "                    amp[j] = res1.x[0]\n",
    "                    diffe[j] = -log_like_F(amp[j], g1[j]) + log_like_F(2000, 1)\n",
    "    \n",
    "    \n",
    "                rows=zip(g1,amp,diffe)\n",
    "                \n",
    "                \n",
    "                with open(f'freq_diff_bin_{bin_size}_{file_id}.dat', 'a') as f:\n",
    "                    writer = csv.writer(f, delimiter=' ')\n",
    "                    for i in rows:\n",
    "                        writer.writerow(i)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc499f52",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Century Gothic', cursive, sans-serif; font-weight: bold; color: purple;font-size: 30px;\">Template for Plots</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff58fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#__________________ For Files in Time domain __________________________\n",
    "\n",
    "file_id_list = []  # to store the file_id\n",
    "slp_dict = {}\n",
    "std_dict = {}\n",
    "bin_sizes = [1,12,48]\n",
    "\n",
    "cwd = os.getcwd() # get the current working directory\n",
    "\n",
    "for size in bin_sizes:\n",
    "    slp_dict[size] = []\n",
    "    std_dict[size] = []\n",
    "    prefix = f\"diff_bin_{size}_\"\n",
    "    \n",
    "    for filename in os.listdir(cwd):\n",
    "        if filename.startswith(prefix):\n",
    "            file_id = filename.split('_')[3].split('.')[0]\n",
    "            file_id_list.append(file_id)\n",
    "            kepler = np.loadtxt(filename)       \n",
    "            slope=kepler[:,0]\n",
    "            ll=kepler[:,2]\n",
    "            slp=np.arange(1., 4.5, 0.1)\n",
    "            llf = np.zeros(len(slope))\n",
    "            length=len(slp)\n",
    "            for i in range(0, len(slope), length):\n",
    "                min_indices = np.where(ll[i:i+length] == np.min(ll[i:i+length]))[0]\n",
    "                random_index = np.random.choice(min_indices)\n",
    "                llf[i] = slope[i:i+length][random_index]\n",
    "                \n",
    "            llf = llf[llf != 0]\n",
    "            slp_dict[size].append(round(np.mean(llf), 2))\n",
    "            std_dict[size].append(round(np.std(llf), 2))\n",
    "\n",
    "\n",
    "file_id_list=np.unique(file_id_list)\n",
    "\n",
    "#__________________ For Files in Frequency domain __________________________\n",
    "\n",
    "file_id_list = []  # to store the file_id\n",
    "slp_dict2 = {}\n",
    "\n",
    "for size in bin_sizes:\n",
    "    slp_dict2[size] = []\n",
    "    prefix = f\"freq_diff_bin_{size}_\"\n",
    "    \n",
    "    for filename in os.listdir(cwd):\n",
    "        if filename.startswith(prefix):\n",
    "            file_id = filename.split('_')[4].split('.')[0]\n",
    "            file_id_list.append(file_id)\n",
    "            kepler = np.loadtxt(filename)       \n",
    "            slope=kepler[:,0]\n",
    "            ll=kepler[:,2]\n",
    "            slp=np.arange(1., 4.5, 0.1)\n",
    "            llf = np.zeros(len(slope))\n",
    "            length=len(slp)\n",
    "            for i in range(0, len(slope), length):\n",
    "                min_indices = np.where(ll[i:i+length] == np.min(ll[i:i+length]))[0]\n",
    "                random_index = np.random.choice(min_indices)\n",
    "                llf[i] = slope[i:i+length][random_index]\n",
    "                \n",
    "            llf = llf[llf != 0]\n",
    "            slp_dict2[size].append(round(np.mean(llf), 2))\n",
    "\n",
    "\n",
    "file_id_list=np.unique(file_id_list)\n",
    "\n",
    "\n",
    "# #______________________________Plot___________________________________________\n",
    "\n",
    "# Set the font size for tick labels\n",
    "plt.rc('xtick', labelsize=30)\n",
    "plt.rc('ytick', labelsize=30)\n",
    "plt.rc('font', size=30)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(35, 12))  # Increase the figsize to make the plots bigger\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "markers = ['o', 's', '^', '*', 'v', 'p', 'D', 'X', 'd', '<', '>', '1', '2', '3', '4', '8', 'h', 'H', '+', 'x', '|']\n",
    "\n",
    "legend_labels = []  # List to store legend labels\n",
    "\n",
    "for i in range(len(slp_dict[1])):\n",
    "    axes[0].scatter([slp_dict2[1][i]],[slp_dict[1][i]], marker=markers[i], label=file_id_list[i],linewidth=10)\n",
    "    axes[1].scatter([slp_dict2[12][i]],[slp_dict[12][i]], marker=markers[i], label=file_id_list[i],linewidth=10)\n",
    "    axes[2].scatter([slp_dict2[48][i]],[slp_dict[48][i]], marker=markers[i], label=file_id_list[i],linewidth=10)\n",
    "\n",
    "    legend_labels.append(file_id_list[i])  # Add legend label to the list\n",
    "\n",
    "axes[0].set_xlabel('PSD slope (frequency domain)')\n",
    "axes[0].set_ylabel('PSD slope (time domain)')\n",
    "axes[0].set_ylim(1., 3.)\n",
    "axes[0].set_xlim(1., 3.)\n",
    "axes[0].set_yticks(np.linspace(1.5, 3, 4))  # Set 5 yticks\n",
    "axes[0].set_title('Bin timescale = 0.5 hours')\n",
    "\n",
    "\n",
    "axes[1].set_xlabel('PSD slope (frequency domain)')\n",
    "axes[1].set_ylim(1., 3.)\n",
    "axes[1].set_xlim(1., 3.)\n",
    "axes[1].set_yticks([])\n",
    "axes[1].set_title('Bin timescale = 6 hours')\n",
    "\n",
    "axes[2].set_xlabel('PSD slope (frequency domain)')\n",
    "axes[2].set_ylim(1., 3.)\n",
    "axes[2].set_xlim(1., 3.)\n",
    "axes[2].set_yticks([])\n",
    "axes[2].set_title('Bin timescale = 24 hours')\n",
    "\n",
    "legend = axes[2].legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=16)\n",
    "legend.set_title(\"Kepler ID\")\n",
    "\n",
    "# Calculating the r and p value\n",
    "\n",
    "r1,p1=np.round(stats.pearsonr(slp_dict2[1],slp_dict[1]),2)\n",
    "r2,p2=np.round(stats.pearsonr(slp_dict2[12],slp_dict[12]),2)\n",
    "r3,p3=np.round(stats.pearsonr(slp_dict2[48],slp_dict[48]),2)\n",
    "\n",
    "axes[0].text(1.7, 2.5, f'$r={r1}, p={p1}$')\n",
    "axes[1].text(1.7, 2.5, f'$r={r2}, p={p2}$')\n",
    "axes[2].text(1.7, 2.5, f'$r={r3}, p={p3}$')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('freq_vs_time_pl.pdf')\n",
    "plt.show()\n",
    "\n",
    "#_____________ Comparing with Smith (Power law case) ___________________________\n",
    "\n",
    "\n",
    "# _______________ Time domain ____________________________\n",
    "\n",
    "\n",
    "smith=np.array([2.4,2.4,2.1,2.8,2,2.7,3.3,1.9,2.8,2.5,2.5,2.4,2.2,3.4,2.5,2.3,2.5,2.4,1.7,3,2.9])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(35, 12))  # Increase the figsize to make the plots bigger\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "markers = ['o', 's', '^', '*', 'v', 'p', 'D', 'X', 'd', '<', '>', '1', '2', '3', '4', '8', 'h', 'H', '+', 'x', '|']\n",
    "\n",
    "legend_labels = []  # List to store legend labels\n",
    "\n",
    "for i in range(len(slp_dict[1])):\n",
    "    axes[0].scatter([smith[i]],[slp_dict[1][i]], marker=markers[i], label=file_id_list[i],linewidth=10)\n",
    "    axes[1].scatter([smith[i]],[slp_dict[12][i]], marker=markers[i], label=file_id_list[i],linewidth=10)\n",
    "    axes[2].scatter([smith[i]],[slp_dict[48][i]], marker=markers[i], label=file_id_list[i],linewidth=10)\n",
    "\n",
    "    legend_labels.append(file_id_list[i])  # Add legend label to the list\n",
    "\n",
    "axes[0].set_xlabel('PSD slope (Smith 2018)')\n",
    "axes[0].set_ylabel('PSD slope (time domain)')\n",
    "axes[0].set_ylim(1., 3.)\n",
    "axes[0].set_xlim(1., 3.5)\n",
    "axes[0].set_yticks(np.linspace(1.5, 3., 4))  # Set 5 yticks\n",
    "axes[0].set_title('Bin timescale = 0.5 hours')\n",
    "\n",
    "\n",
    "axes[1].set_xlabel('PSD slope (Smith 2018)')\n",
    "axes[1].set_ylim(1., 3.)\n",
    "axes[1].set_xlim(1., 3.5)\n",
    "axes[1].set_yticks([])\n",
    "axes[1].set_title('Bin timescale = 6 hours')\n",
    "\n",
    "axes[2].set_xlabel('PSD slope (Smith 2018)')\n",
    "axes[2].set_ylim(1., 3.)\n",
    "axes[2].set_xlim(1., 3.5)\n",
    "axes[2].set_yticks([])\n",
    "axes[2].set_title('Bin timescale = 24 hours')\n",
    "\n",
    "legend = axes[2].legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=16)\n",
    "legend.set_title(\"Kepler ID\")\n",
    "\n",
    "# Calculating the r and p value\n",
    "\n",
    "r4,p4=np.round(stats.pearsonr(smith,slp_dict[1]),2)\n",
    "r5,p5=np.round(stats.pearsonr(smith,slp_dict[12]),2)\n",
    "r6,p6=np.round(stats.pearsonr(smith,slp_dict[48]),2)\n",
    "\n",
    "axes[0].text(1.7, 2.5, f'$r={r4}, p={p4}$')\n",
    "axes[1].text(1.7, 2.5, f'$r={r5}, p={p5}$')\n",
    "axes[2].text(1.2, 2.3, f'$r={r6}, p={p6}$')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('time_vs_smith_pl.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ____________ FREQUENCY DOMAIN ________________________\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(35, 12))  # Increase the figsize to make the plots bigger\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "markers = ['o', 's', '^', '*', 'v', 'p', 'D', 'X', 'd', '<', '>', '1', '2', '3', '4', '8', 'h', 'H', '+', 'x', '|']\n",
    "\n",
    "legend_labels = []  # List to store legend labels\n",
    "\n",
    "for i in range(len(slp_dict[1])):\n",
    "    axes[0].scatter([smith[i]],[slp_dict2[1][i]], marker=markers[i], label=file_id_list[i],linewidth=10)\n",
    "    axes[1].scatter([smith[i]],[slp_dict2[12][i]], marker=markers[i], label=file_id_list[i],linewidth=10)\n",
    "    axes[2].scatter([smith[i]],[slp_dict2[48][i]], marker=markers[i], label=file_id_list[i],linewidth=10)\n",
    "\n",
    "    legend_labels.append(file_id_list[i])  # Add legend label to the list\n",
    "\n",
    "axes[0].set_xlabel('PSD slope (Smith 2018)')\n",
    "axes[0].set_ylabel('PSD slope (frequency domain)')\n",
    "axes[0].set_ylim(1., 3.)\n",
    "axes[0].set_xlim(1., 3.5)\n",
    "axes[0].set_yticks(np.linspace(1.5, 3., 4))  # Set 5 yticks\n",
    "axes[0].set_title('Bin timescale = 0.5 hours')\n",
    "\n",
    "\n",
    "axes[1].set_xlabel('PSD slope (Smith 2018)')\n",
    "axes[1].set_ylim(1., 3.)\n",
    "axes[1].set_xlim(1., 3.5)\n",
    "axes[1].set_yticks([])\n",
    "axes[1].set_title('Bin timescale = 6 hours')\n",
    "\n",
    "axes[2].set_xlabel('PSD slope (Smith 2018)')\n",
    "axes[2].set_ylim(1., 3.)\n",
    "axes[2].set_xlim(1., 3.5)\n",
    "axes[2].set_yticks([])\n",
    "axes[2].set_title('Bin timescale = 24 hours')\n",
    "\n",
    "legend = axes[2].legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=18)\n",
    "legend.set_title(\"Kepler ID\")\n",
    "\n",
    "\n",
    "# Calculating the r and p value\n",
    "\n",
    "r7,p7=np.round(stats.pearsonr(smith,slp_dict2[1]),2)\n",
    "r8,p8=np.round(stats.pearsonr(smith,slp_dict2[12]),2)\n",
    "r9,p9=np.round(stats.pearsonr(smith,slp_dict2[48]),2)\n",
    "\n",
    "axes[0].text(1.7, 2.5, f'$r={r7}, p={p7}$')\n",
    "axes[1].text(1.7, 2.5, f'$r={r8}, p={p8}$')\n",
    "axes[2].text(1.2, 2.5, f'$r={r9}, p={p9}$')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('freq_vs_smith_pl.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#___________ Plotting TIME & FREQ for 1 object ____________________\n",
    "\n",
    "file_id_list = []  # to store the file_id\n",
    "slp_dict = {}\n",
    "ll_dict = {}\n",
    "bin_sizes = [48]\n",
    "\n",
    "cwd = os.getcwd()  # get the current working directory\n",
    "\n",
    "\n",
    "for size in bin_sizes:\n",
    "    slp_dict[size] = []\n",
    "    ll_dict[size] = []\n",
    "    prefix = f\"diff_bin_{size}\"\n",
    "\n",
    "    for filename in os.listdir(cwd):\n",
    "        if filename.startswith(prefix):\n",
    "            file_id = filename.split('_')[3].split('.')[0]\n",
    "            file_id_list.append(file_id)\n",
    "            kepler = np.loadtxt(filename)\n",
    "            slope = kepler[:, 0]\n",
    "            ll = kepler[:, 2]\n",
    "            slp_dict[size].append(slope)\n",
    "            ll_dict[size].append(ll)\n",
    "\n",
    "file_id_list = np.unique(file_id_list)\n",
    "\n",
    "\n",
    "ll2_dict = {}\n",
    "\n",
    "# Define the figure size\n",
    "plt.figsize = (20,20)  # Adjust the values as needed\n",
    "# Set the font size for tick labels\n",
    "plt.rc('xtick', labelsize=30)\n",
    "plt.rc('ytick', labelsize=30)\n",
    "plt.rc('font', size=30)\n",
    "\n",
    "\n",
    "for size in bin_sizes:\n",
    "    ll2_dict[size] = []\n",
    "    prefix = f\"freq_diff_bin_{size}\"\n",
    "\n",
    "    for filename in os.listdir(cwd):\n",
    "        if filename.startswith(prefix):\n",
    "            kepler = np.loadtxt(filename)\n",
    "            ll2 = kepler[:, 2]\n",
    "            ll2_dict[size].append(ll2)\n",
    "\n",
    "# Create separate plots for each file_id\n",
    "for file_id in file_id_list:\n",
    "    plt.figure()  # Create a new figure for each file_id\n",
    "    for size in bin_sizes:\n",
    "        for i in range(len(slp_dict[size])):\n",
    "            if file_id in file_id_list[i]:\n",
    "                plt.figure(figsize=(20, 20))\n",
    "                plt.title(f'Kepler ID: {file_id}')\n",
    "                plt.plot(slp_dict[size][i], ll_dict[size][i], linewidth=6,color='r',label='Time domain')\n",
    "                plt.plot(slp_dict[size][i], ll2_dict[size][i], linewidth=6,color='b',label='Frequency domain')\n",
    "                plt.ylim(1.8*np.min(ll2),100)\n",
    "                plt.ylabel('$ - \\ln \\mathcal{L}(F)$')\n",
    "                plt.xlabel(r'$\\gamma$', fontsize=30.0)\n",
    "                plt.legend(loc='center')\n",
    "                \n",
    "                # Generate a unique filename based on file_id\n",
    "                filename = f'time_vs_freq_{file_id}_{size}_{i}.pdf'\n",
    "                # Check if the filename starts with \"578\" before saving\n",
    "                if file_id.startswith('578'):\n",
    "                \n",
    "                # Save the figure with the unique filename\n",
    "                    plt.savefig(filename)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148b265",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Verdana', cursive, sans-serif; font-weight: bold; color: blue;font-size: 40px;\">For The Broken Power Law PSD Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe28f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() # get the current working directory\n",
    "\n",
    "for filename in os.listdir(cwd):\n",
    "    if filename.startswith('lcwerrors'):\n",
    "        file_id = filename.split('_')[1].split('.')[0]\n",
    "        kepler = np.loadtxt(filename) \n",
    "        \n",
    "       \n",
    "        t=np.round(kepler[:,0],4)\n",
    "        lc=kepler[:,1]\n",
    "\n",
    "        step=np.round(np.diff(t),4)\n",
    "        \n",
    "        bin_sizes=np.array([1])\n",
    "        \n",
    "        \n",
    "        for bin_size in bin_sizes:\n",
    "            \n",
    "            \n",
    "            points=len(lc)\n",
    "            tolerance=90000.05 \n",
    "            cadence=np.min(step)\n",
    "\n",
    "\n",
    "            # +1 is added so that the splitting is done after the last element which \n",
    "            # satisfies the condition. if not added, the last element will not be \n",
    "            # included in the range. \n",
    "    \n",
    "            seq = np.split(t, np.where(step > tolerance*np.min(step))[0] +1)\n",
    "    \n",
    "            l = []\n",
    "    \n",
    "            for s in seq:\n",
    "                if len(s) >= points:\n",
    "                    l.append((np.min(s), s[-1]))\n",
    "                    \n",
    "            for x in range (len(l)):\n",
    "                    \n",
    "            # section to find the starting and the end indices of the time segments\n",
    "                    \n",
    "                def t_ini(x):\n",
    "                \n",
    "                    starting_index=int(np.where(t==np.min(l[x]))[0])\n",
    "                \n",
    "                    return starting_index\n",
    "    \n",
    "                def t_fin(x):\n",
    "                \n",
    "                    ending_index=int(np.where(t==np.max(l[x]))[0])\n",
    "                \n",
    "                    return ending_index\n",
    "    \n",
    "    \n",
    "    \n",
    "                t_i=t_ini(x)\n",
    "                t_f=t_fin(x)    \n",
    "                \n",
    "                \n",
    "                f=interpolate.interp1d(t[t_i:t_f+1],lc[t_i:t_f+1],kind='linear',fill_value='extrapolate')\n",
    "                       \n",
    "                # # evenly spaced time series segments\n",
    "                  \n",
    "                T=np.arange(t[t_i],t[t_f],cadence)\n",
    "    \n",
    "                # new flux after interpolation\n",
    "    \n",
    "                Ft_=f(T)\n",
    "                   \n",
    "                index=int(points//bin_size)*bin_size # index upto which Ft1 is divisible by binning size\n",
    "                   \n",
    "                Ft1=Ft_[:index]\n",
    "           \n",
    "    \n",
    "                Ft=np.mean(Ft1.reshape(-1,bin_size),axis=1)\n",
    "                \n",
    "                F_w = np.fft.fft(Ft, norm='ortho')\n",
    "            \n",
    "                N=len(Ft)\n",
    "                 \n",
    "                w=2*(np.pi)*np.linspace(1,N,N)/N/cadence\n",
    "                \n",
    "                def power (gamma1,gamma2,w0):\n",
    "                    \n",
    "                  p=np.zeros(len(w))\n",
    "                        \n",
    "                  for i in range(len(w)):\n",
    "                      if (w[i]<=w0):\n",
    "                          p[i] = (w[i]/w0)**(-gamma1)\n",
    "                      else:\n",
    "                          p[i] = (w[i]/w0)**(-gamma2)\n",
    "                                \n",
    "                  return p\n",
    "                \n",
    "            # Noise level prescription\n",
    "            \n",
    "                sigma_F=np.abs(np.mean(np.abs((np.diff(Ft))))/np.sqrt(bin_size))   \n",
    "                \n",
    "                def Sigma_phi(alpha,gamma1,gamma2,w0):\n",
    "                    Sigma_phi_ = (alpha**2) * power(gamma1,gamma2,w0)\n",
    "                    return Sigma_phi_\n",
    "                \n",
    "                \n",
    "                def log_like_F (alpha,gamma1,gamma2,w0):\n",
    "            \n",
    "                    Sigma_phi_ = Sigma_phi(alpha,gamma1,gamma2,w0)\n",
    "                    \n",
    "                    power_F_ = (np.abs(F_w)**2)\n",
    "                    \n",
    "                    lh = -0.5*(np.sum(power_F_ / Sigma_phi_) + np.sum(np.log(2*np.pi*Sigma_phi_)))\n",
    "                    \n",
    "                    return lh\n",
    "                \n",
    "                \n",
    "                def mnz(args):\n",
    "                    alpha, gamma1, gamma2, w0 = args\n",
    "                    return -log_like_F(alpha,gamma1,gamma2,w0) + log_like_F(5000,1,1,1)\n",
    "    \n",
    "    \n",
    "                # Frequency range \n",
    "                \n",
    "                t_low, t_high =0.125, 100  # DAYS\n",
    "                freq_step=1/(1*t[-1])\n",
    "    \n",
    "                wbk = np.arange(1/t_high,1/t_low,freq_step)\n",
    "                \n",
    "    \n",
    "                diffe = np.zeros(len(wbk))\n",
    "                amp = np.zeros(len(wbk))\n",
    "                ind1 = np.zeros(len(wbk))\n",
    "                ind2 = np.zeros(len(wbk))\n",
    "    \n",
    "                results = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "                  delayed(minimize)(mnz, (10000,1,2.5,wbk_), bounds=((100, 1000000),(1,1),(1,4),(wbk_, wbk_)))\n",
    "                  for wbk_ in wbk\n",
    "                )\n",
    "    \n",
    "                for j, res1 in enumerate(results):\n",
    "                    amp[j] = res1.x[0]\n",
    "                    ind1[j]=float(res1.x[1])\n",
    "                    ind2[j]=float(res1.x[2])\n",
    "                    diffe[j] = -log_like_F(amp[j],ind1[j],ind2[j],wbk[j]) + log_like_F(5000,1,1,1)\n",
    "    \n",
    "    \n",
    "                rows=zip(wbk,amp,ind2,diffe)\n",
    "                \n",
    "                \n",
    "                with open(f'freq_bpl_diff_bin_{bin_size}_{file_id}.dat', 'w') as f:\n",
    "                    writer = csv.writer(f, delimiter=' ')\n",
    "                    for i in rows:\n",
    "                        writer.writerow(i)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c369751",
   "metadata": {},
   "source": [
    "# <span style=\"font-family: 'Century Gothic', cursive, sans-serif; font-weight: bold; color: purple;font-size: 30px;\">Template for Plots</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#____________ Obtaining the break freq and the slope _________________\n",
    "\n",
    "file_id_list = []  # to store the file_id\n",
    "slp_dict = {}\n",
    "w_dict = {}\n",
    "bin_sizes = [1]\n",
    "\n",
    "cwd = os.getcwd()  # get the current working directory\n",
    "\n",
    "for size in bin_sizes:\n",
    "    slp_dict[size] = []\n",
    "    w_dict[size] = []\n",
    "    prefix = f\"freq_bpl_diff_bin_{size}_\"\n",
    "\n",
    "    for filename in os.listdir(cwd):\n",
    "        if filename.startswith(prefix):\n",
    "            file_id = filename.split('_')[5].split('.')[0]\n",
    "            kepler = np.loadtxt(filename)\n",
    "            w = kepler[:, 0]\n",
    "            slope = kepler[:, 2]\n",
    "            ll = kepler[:, 3]\n",
    "            llf, llf2 = np.zeros(len(w)), np.zeros(len(w))\n",
    "            length = len(w)\n",
    "            for i in range(0, len(w), length):\n",
    "                min_indices = np.where(ll[i:i + length] == np.min(ll[i:i + length]))[0]\n",
    "                random_index = np.random.choice(min_indices)\n",
    "                if random_index > 0:\n",
    "\n",
    "                    llf[i] = slope[random_index]\n",
    "                    llf2[i] = w[random_index]\n",
    "\n",
    "\n",
    "            llf = llf[llf != 0]\n",
    "            if len(llf) > 0 and llf[i] > 1.0:  # Only append if there are valid values\n",
    "                slp_dict[size].append(round(np.mean(llf), 2))\n",
    "                w_dict[size].append(round(llf2[0], 4))\n",
    "                file_id_list.append(file_id)\n",
    "                \n",
    "                \n",
    "#__________ Correlation with BH mass, Lum, Edd ratio with the break timescales\n",
    "\n",
    "\n",
    "# _______Black Hole Mass __________\n",
    "\n",
    "\n",
    "bh = np.array([7.58, 7.52, 7.8, 7.38, np.nan, np.nan, 8.59, \n",
    "               7.9, 7.66, 8.49, 7.73, 7.43, np.nan, 7.3, 7.37])\n",
    "\n",
    "bh_nan = np.isnan(bh)\n",
    "w_bh = np.array(w_dict[1])[~bh_nan]\n",
    "bh2 = bh[~bh_nan]\n",
    "file_bh=np.array(file_id_list)[~bh_nan]\n",
    "\n",
    "# _______ Luminosity _____________________\n",
    "\n",
    "lum = np.array([44.77, 44.23, 44.32, 44.36, np.nan, np.nan, 45.78, 45.01, 44.71, 45.74, \n",
    "       44.18, 44.66, 44.95, 44.14, 44.4])\n",
    "\n",
    "lum_nan = np.isnan(lum)\n",
    "w_lum = np.array(w_dict[1])[~lum_nan]\n",
    "lum2 = lum[~lum_nan]\n",
    "file_lum=np.array(file_id_list)[~lum_nan]\n",
    "\n",
    "\n",
    "#__________ Eddington ratio ________________\n",
    "\n",
    "\n",
    "edd = np.array([0.124, 0.04, 0.026, 0.076, np.nan, np.nan, 0.124, 0.101, 0.089, \n",
    "       0.14, 0.023, 0.135, np.nan, 0.055, 0.085])\n",
    "\n",
    "edd_nan = np.isnan(edd)\n",
    "w_edd = np.array(w_dict[1])[~edd_nan]\n",
    "edd2 = edd[~edd_nan]\n",
    "file_edd=np.array(file_id_list)[~edd_nan]\n",
    "                               \n",
    "r1,p1=np.round(stats.pearsonr(bh2,1/w_bh),2)   \n",
    "r2,p2=np.round(stats.pearsonr(lum2,1/w_lum),2)  \n",
    "r3,p3=np.round(stats.pearsonr(edd2,1/w_edd),2)  \n",
    "\n",
    "# Set the font size for tick labels\n",
    "plt.rc('xtick', labelsize=30)\n",
    "plt.rc('ytick', labelsize=30)\n",
    "plt.rc('font', size=30)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(35, 12))  # Increase the figsize to make the plots bigger\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "\n",
    "\n",
    "axes[0].scatter(bh2,1/w_bh, marker='D', color='r', linewidth=20)\n",
    "axes[1].scatter(lum2,1/w_lum, marker='D', color='r', linewidth=20)\n",
    "axes[2].scatter(edd2,1/w_edd, marker='D', color='r', linewidth=20)\n",
    "\n",
    "\n",
    "axes[0].set_xlabel(r'$\\log M_{\\mathrm{BH}}\\ (M_{\\odot})$')\n",
    "axes[0].set_ylabel(r'$T_{break}$ (Days)')\n",
    "\n",
    "axes[1].set_xlabel(r'$\\log L_{Bol}$ (erg $s^{-1}$)')\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "axes[2].set_xlabel(r'$L/L_{Edd}$')\n",
    "axes[2].set_yticks([])\n",
    "\n",
    "# Setting 5 yticks for axes[0]\n",
    "\n",
    "min_w_bh = int(np.min(1/w_bh))\n",
    "max_w_bh = int(np.max(1/w_bh))\n",
    "\n",
    "# Calculate the spacing between ticks\n",
    "tick_spacing = (max_w_bh - min_w_bh) / 4  # Divide by 4 to get 5 ticks\n",
    "\n",
    "# Generate a list of 5 ytick values evenly spaced between min and max\n",
    "ytick_values = np.arange(min_w_bh, max_w_bh + tick_spacing, tick_spacing)\n",
    "\n",
    "# Set the yticks for axes[0]\n",
    "axes[0].set_yticks(ytick_values)\n",
    "\n",
    "\n",
    "axes[0].text(7.5, 53, f'$r={r1}, p={p1}$')\n",
    "axes[1].text(44.5, 53, f'$r={r2}, p={p2}$')\n",
    "axes[2].text(0.05, 53, f'$r={r3}, p={p3}$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('break_agn_prop.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# _______ Plotting log likelihood with break freq for a single object ___________\n",
    "\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=35) \n",
    "matplotlib.rc('ytick', labelsize=35) \n",
    "plt.rc('font', size=45)\n",
    "\n",
    "for size in bin_sizes:\n",
    "    slp_dict[size] = []\n",
    "    w_dict[size] = []\n",
    "    prefix = f\"freq_bpl_diff_bin_{size}_12010193\"\n",
    "\n",
    "    for filename in os.listdir(cwd):\n",
    "        if filename.startswith(prefix):\n",
    "            file_id = filename.split('_')[5].split('.')[0]\n",
    "            kepler = np.loadtxt(filename)\n",
    "            w = kepler[:, 0]\n",
    "            slope = kepler[:, 2]\n",
    "            ll = kepler[:, 3]\n",
    "            \n",
    "            \n",
    "            plt.figure(figsize=(25, 25))\n",
    "            plt.plot(w, ll, linewidth=4, color='k')\n",
    "            plt.title(f'Kepler ID: {file_id}')\n",
    "            \n",
    "            plt.ylabel('$ - \\ln \\mathcal{L}(F)$')\n",
    "            plt.xlabel('$\\omega_{bk}$ (rad/day)')\n",
    "            plt.xscale('log')\n",
    "            plt.savefig(f'bpl_{file_id}_wbk.pdf')\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "\n",
    "#__________ Plotting the plane connecting M_BH, Lum and T_break ____________\n",
    "\n",
    "file_id_list = []  # to store the file_id\n",
    "slp_dict = {}\n",
    "w_dict = {}\n",
    "bin_sizes = [1]\n",
    "\n",
    "cwd = os.getcwd()  # get the current working directory\n",
    "\n",
    "for size in bin_sizes:\n",
    "    slp_dict[size] = []\n",
    "    w_dict[size] = []\n",
    "    prefix = f\"freq_bpl_diff_bin_{size}_\"\n",
    "\n",
    "    for filename in os.listdir(cwd):\n",
    "        if filename.startswith(prefix):\n",
    "            file_id = filename.split('_')[5].split('.')[0]\n",
    "            kepler = np.loadtxt(filename)\n",
    "            w = kepler[:, 0]\n",
    "            slope = kepler[:, 2]\n",
    "            ll = kepler[:, 3]\n",
    "            llf, llf2 = np.zeros(len(w)), np.zeros(len(w))\n",
    "            length = len(w)\n",
    "            for i in range(0, len(w), length):\n",
    "                min_indices = np.where(ll[i:i + length] == np.min(ll[i:i + length]))[0]\n",
    "                random_index = np.random.choice(min_indices)\n",
    "                if random_index > 0:\n",
    "\n",
    "                    llf[i] = slope[random_index]\n",
    "                    llf2[i] = w[random_index]\n",
    "\n",
    "\n",
    "            llf = llf[llf != 0]\n",
    "            if len(llf) > 0 and llf[i] > 1.0:  # Only append if there are valid values\n",
    "                slp_dict[size].append(round(np.mean(llf), 2))\n",
    "                w_dict[size].append(round(llf2[0], 4))\n",
    "                file_id_list.append(file_id)\n",
    "                    \n",
    "\n",
    "matplotlib.rc('xtick', labelsize=35) \n",
    "matplotlib.rc('ytick', labelsize=35) \n",
    "plt.rc('font', size=45)          \n",
    "                \n",
    "bh = np.array([7.58, 7.52, 7.8, 7.38, np.nan, np.nan, 8.59, \n",
    "               7.9, 7.66, 8.49, 7.73, 7.43, np.nan, 7.3, 7.37])\n",
    "\n",
    "\n",
    "lum = np.array([44.77, 44.23, 44.32, 44.36, np.nan, np.nan, 45.78, 45.01, 44.71, 45.74, \n",
    "       44.18, 44.66, np.nan, 44.14, 44.4])\n",
    "\n",
    "bh_nan = np.isnan(bh)\n",
    "lum_nan = np.isnan(lum)\n",
    "\n",
    "x1= bh[~bh_nan]\n",
    "y1= lum[~lum_nan]\n",
    "z1 = 1/np.array(w_dict[1])[~lum_nan]\n",
    "\n",
    "X_data = np.column_stack((x1,y1))\n",
    "Y_data = z1\n",
    "\n",
    "reg = linear_model.LinearRegression().fit(X_data, Y_data)\n",
    "\n",
    "alpha1=reg.coef_[0]\n",
    "alpha2=reg.coef_[1]\n",
    "alpha3=reg.intercept_\n",
    "\n",
    "alpha1=reg.coef_[0]\n",
    "alpha2=reg.coef_[1]\n",
    "alpha3=reg.intercept_\n",
    "\n",
    "wbk=alpha1*x1[len(x1)-1]+alpha2*y1[len(y1)-1]+alpha3\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=20) \n",
    "matplotlib.rc('ytick', labelsize=20) \n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.tick_params(axis='x', which='major', pad=-40)\n",
    "ax.tick_params(axis='y', which='major', pad=-25)\n",
    "ax.tick_params(axis='z', which='major', pad=-25)\n",
    "\n",
    "ax.locator_params(axis='y', nbins=3)\n",
    "ax.locator_params(axis='x', nbins=3)\n",
    "ax.locator_params(axis='z', nbins=3)\n",
    "  \n",
    "ax.scatter(x1,y1,z1,marker='*',linewidth=25,c='r')\n",
    "ax.set_xlabel(r'log $M_{BH}$ ($M_{\\odot}$)',fontsize=35)\n",
    "ax.set_ylabel(r'log $L_{Bol}$  (erg $s^{-1}$)',fontsize=35)\n",
    "ax.set_zlabel('$T_{break}(Days)$',fontsize=35)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.setp(ax.get_yticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Plot the best fit plane\n",
    "\n",
    "xx, yy = np.meshgrid(x1, y1)\n",
    "zz=alpha1*xx+alpha2*yy+alpha3\n",
    "ax.plot_surface(xx, yy, zz, alpha=0.02,color='g')\n",
    "#ax.text(6.7,44.5,60, r'$T = {alpha1} M_{BH} - {alpha2} L_{Bol} + {alpha3}$',fontsize=35)\n",
    "# Format the text to display the variable values\n",
    "text = r'$T = {:.2f} M_{{BH}} + {:.2f} L_{{Bol}}  {:.2f}$'.format(alpha1, alpha2, alpha3)\n",
    "ax.text(7.5, 44.5, 80, text, color='b',fontsize=35)\n",
    "plt.show()\n",
    "fig.savefig('plane_break.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
